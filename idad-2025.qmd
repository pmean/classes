---
title: "idad 2025 notes"
format: html
editor: source
---

## Deep learning

2012 AlexNet ImageNet competition

2015 DeepMind go

2018 Turing Prize, Bengio, hinton, LeCun

2024 Nobel Prize in Physics Hopfield and Hinton.

Deep forward networks or multilayer perceptrons

Boosting was his favorite.

Neural nets can do supervised or unsupervised learning

Prediction is more important than inference.

Feature extraction (HOG, SIFT), Post-processing (MKL)

End to end recognition system

Composition is at the core of deep learning systems.

z = Wx + b
a = sigma(z)
One choice is max(x, 0) (Rectified Linear Unit)

Deep means more than two layers

Forward propogation (what is error?)

Back propogation (Gradient descent)

Stochastic gradient descent (looks at a subset of the data) Use first order instead of Newton's method.

w <- W - nu dL/dW

Uses the chain rule. This a non-convex loss function

Use cross entropy

soft max=exp(Zi)/sum(exp(Zi))

KL distance

Classification, object detection, semantic segmentation, image captioning.

What I cannot create, I do not understand.

Given training data, generate new samples from the same distribution.

Want to learning P_model(x) similar to P_data(x)

CELEBA-HQ database

Explicit versus implicit density estimation.

Generative means that you don't get the same answer each time.

Flow based model

Affine coupling layer, real-valued non-volume preserving

yd+1"D=xd+1:D exp(s(X1:d) +t)x1:d)

GAN

Generator network versus Discriminator network

Jensen-Shannon divergence

Are GANs created equal a large scale study Lucic

Wasserstein distance, Wasserstein GAN

torch torchvision scikit-learn pandas

sklearn microwave pre-packaged food, torch cook your own food

Generative AI is what we used to call unsupervised learning.

What is "highly" nonlinear

The universal approximation theorem (Hornik 1989, Cybenko 1989)
SGD Stochastic Gradient Function

yin164@purdue.edu
