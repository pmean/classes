---
title: "Test-retest and interrater reliability"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---


## Test-retest reliability

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE)
suppressMessages(suppressWarnings(library(tidyverse)))
```

```{r image}
data.frame(x=0:1, y=0:1) %>%
  ggplot(aes(x, y)) +
  xlab(" ") + 
  ylab(" ") +
  geom_label(x=0.5, y=0.5, label="Measurement\n1") +
  geom_label(x=0.8, y=0.5, label="Measurement\n2") +
  geom_text(x=0.5, y=0.2, label="Time 1") +
  geom_text(x=0.8, y=0.2, label="Time 2") +
  geom_text(x=0.2, y=0.5, label="Observer 1") +
  theme_void()
```

::: notes

Wow. That's a lot to digest. Don't be afraid to ask me questions about reliability. 

We talked about measures of internal consistency and why I don't like them. We also talked about some practical advice: report reliability measures from your literature review and measure reliability, if you can, in your current study.

:::

## Test-retest reliability (also called repeatability)
-   Correlation of two measurements separated by time
-   Length of time interval is critical
    -   No carry-over
    -   No changes in the true score
-   Useful for composite scores and single values
-   Useful for self-report and researcher evaluation
-   Not possible for some measures

::: notes

Test-retest reliability is the correlation coefficient of two measurements taken at different times. This is also known as repeatability.

The correlation coefficient between the two measurments is an estimate of the reliability coefficient.

The time interval is critical here. You don't want two measurements that are so close together that the measurement error for the first measurement is correlated with the measurement error of the second measurement.

Suppose you are measuring your patient's knowledge about a disease. If you give the same test only a few minutes apart, your patient will remember his/her answers to the first test when answering the second test.

So you want a long enough time interval that there is no carry over effect.

But too large an interval is also problematic. You want to make sure that the true score is the same (or very close)

Over what size interval would you expect the measure to be stable? It depends on what you are measuring. Intelligence is likely to be stable along long time frames but mood changes rapidly.

Test-retest reliability works well for any type of measure. But it can't always be applied. The Apgar score has to be measured at the time of birth. If the baby is blue, limp, and unresponsive after three hours, that's not quite the same as the same occuring right after birth.

:::

## Inter-rater reliability

```{r image-2}
data.frame(x=0:1, y=0:1) %>%
  ggplot(aes(x, y)) +
  xlab(" ") + 
  ylab(" ") +
  geom_label(x=0.5, y=0.5, label="Measurement\n1") +
  geom_label(x=0.5, y=0.8, label="Measurement\n2") +
  geom_text(x=0.5, y=0.2, label="Time 1") +
  geom_text(x=0.2, y=0.5, label="Observer 1") +
  geom_text(x=0.2, y=0.8, label="Observer 2") +
  theme_void()
```

## Inter-rater reliability

-   Used for researcher evaluations only
-   Simplest case
    -   Two independent raters
    -   Ratings for every patient
-   Analysis
    -   Intraclass correlation or Cohen's Kappa
-   Extensions

::: notes

When the researcher does the evaluation and there is concern that a subjective element may creep in and cause measurement error. Your observed score might be higher or lower depending only on who is rating you.

Reliability is pretty simple to measure in this setting, if you have the resources. Just get two raters and have them both compute the measurement. If the correlation between the two raters is high, you have good reliability.

Rather than computing a direct correlation, inter-rater reliability is usually computed as an intra-class correlation. The intraclass correlation generalizes naturally to more complex settings.

If your measurement is binary (note the entire measurement is binary, which is not the same as saying that the individual components of a composite score are binary), then a different statistic, Cohen's Kappa is used. Like the intraclass correlation, there are extensions of Kappa to multiple raters.

You can't always have all the raters rate all the patients, especially if you have more than two raters. There are extensions to cases where you have random assignments of patients to different raters, but the formulas are tricky.

:::

## Practical guidance on reliability

-   Is there previous literature?
    -   Report their reliability coefficients
-   Is your setting similar?
    -   Different demographics?
    -   Different cultural norms?
    -   Different literacy?
    -   Different language?
-   Compare to reliability in your sample
    -   Test-retest and inter-rater reliability preferred.
    -   0.7 or higher

::: notes

You should include a discussion of reliability in your literature review. Cite the reliability coefficients in previous work, as it adds to the credibility of your proposed research.

But take a step back and ask if you can extrapolate safely to the research setting that you propose. Recall the hypothetical reliability coefficient. It compared the variation of the true score to the variation of the observed score across patients in the population you are studying. If your population is quite different than the populations in your literature review, you have no guarantee that a measurement proven to be reliable in previous work will continue to stay reliable in your setting.

Some differences to look for are differences in the demographics of your population, differences in cultural norms and expectations, differences in literacy levels (especially for measurements that require your patients to read and respond to a questionnaire).

If you are measuring something that requires translation to a different language, keep in mind that not all concepts translate well from one language to another. Sometimes it helps to pay for a second and independent translation back to the original language. If there are discrepancies, then maybe it was in the back-translation, but more likely, you are asking for a different type of information in your new language without realizing it.

If you can, incorporate a measure of reliability into your study. There are two reasons for this. First, your setting may be different enough to raise concerns. Getting a current measure of reliability helps to allay those concerns. Second, reliability is never quite perfect, because all of the measures of reliability are indirect measures. Your effort to assess reliability will supplement the previous work on reliability and make things a bit easier for future researchers.

I have a strong preference for test-restest reliability or inter-rater reliability, if you can get it.  The other measures of reliability, parallel forms, the split half correlation, Kuder-Richardson 20, and Cronbach's alpha are measures of the internal homogeneity of your composite measure. In my mind they are a poor substitute for test-retest reliability or inter-rater reliability.

I do not like these measures. Let me restate that. I despise these measures. They are simplistic and fail to measure what I think are the important features of reliability (stability over time and consistency between raters). I think people use them mindlessly and fail to recognize that they are measuring something very limited.

If you can't measure reliability using a test-retest approach or using inter-rater reliability, then go ahead and use these other approaches. But they are a pale substitute in my opinion.

The general target value for a reliability coefficient is 0.7 or higher. You might get by with a reliability coefficient of 0.6, but don't count on it.

:::

