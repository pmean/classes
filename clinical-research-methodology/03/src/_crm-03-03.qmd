---
title: "Summarizing and evaluating individual articles"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## Summarizing

-   No direct quotes.
-   Contextual clues
    -   "Also", "In addition"
    -   "In contrast", "On the other hand"
  
::: notes
This is not a hard and fast rule, but you generally want to avoid direct quotes in a literature review. In other places, direct quotes are fine, but in a literature review, it makes the writing unwieldy. 

One of the websites I visited to prepare this lecture had some interesting advice about transitional words. I lost track of which site (shame, shame on me for not keeping better track of things!). The point that they made is that a long list, study after study can easily lead the reader to feel lost in all the details. Place each study in context relative to the previously mentioned. If the study is providing further support to the results of the previous study, emphasize this with words like "also" or "in addition". If the study contrasts or contradicts the previous study, emphasize this using words like "in contrast" or "on the other hand". This is a little thing, but it does help.
:::

## Example of contextual words

"Dosage compensation in mammalian females is a recognized phenomenon whereby inactivation of one X chromosome ...[1]. **However,** not all X-linked genes are inactivated. Recently, an inactivation profile...was reported by Carrel and Willard [2]. ... **Subsequently**, Lyon [3] ... enhanced our knowledge about X-chromosome inactivation..."

    -   Talebizadeh et al. X chromosome gene expression in human tissues: Male and female comparisons. Genomics 2006.

::: notes
Notice the contextual words here. The second reference implies something quite different than the first.

The third reference supports the perspective of the second.

Note: I'm citing my own paper here, not because the writing is better than anyone else's, but rather because it's easier to talk about something you are already familiar with.
:::

## Analyzing

-   More than just the abstract
    -   Inconsistencies with main text
   -   Misplaced emphasis in abstract
-   Research types
    -   Comparative
    -   Associational
    -   Descriptive (Estimate, Identify)
-   Put in some passion

::: notes

You can't rely on just the abstract. It's fine for tossing out papers that obviously don't fit. But the abstract is sometimes inconsistent with the text. It's easy to do--you write your  paper, then you write your abstract, then you make a last minute change in the paper and forget to update your abstract.

There's also lots of documented examples of misplaced emphasis in the abstract. The text of a paper might identify an outcome that is primary, but the abstract might talk instead about a secondary outcome because it was the only one with a small p-value.

So please read the entire paper.

Most research involves comparison, so try to use words that emphasize superiority like "improved" or "better" or that emphasize comparability like "similar" or "equivalent".

Your analysis will be quite different, of course, for descriptive studies. For these, you offer an estimate (such as "1.2 million people in the United States have diabetes") or list one or more features of a group (such as "the most commonly used drugs in transplant patients to prevent rejection are calcineurin inhibitors, antiproliferative agents, and steroids").

Don't try to be dispassionate in your analysis. It is okay to invoke emotions like surprise or disappointment. Provide guidance to magnitude with phrases like "substantial change" or "moderate improvement". You can editorialize with phrases like "landmark study". Some people overdo this, but in my experience, too many authors are too timid about their writing. The desire to appear impartial causes them to suck all the life out of their prose.

Even if others disagree with your perspective, you have given them something to react to. This is far better than expecting the reader to figure out from subtle hints what you really think.

:::

## Example of descriptive and comparative analysis

"African Americans represent nearly 45% of new HIV cases each year (1–2). Due to delayed HIV diagnosis, African Americans tend to enter HIV treatment at advanced stages and die from AIDS sooner than Whites (1)."

    -   Berkley-Patton et al. An HIV Testing Intervention in African American Churches: Pilot Study Findings. Ann Behav Med. 2016. 
 
::: notes
This is a nice example of description where you provide an estimate (45%) followed by a comparison (advanced stages, die sooner).
:::


## Example of descriptive analysis

"Studies suggest many African American faith leaders are willing to provide HIV education ...(14–17); however, their reported challenges in doing so have included church capacity issues (e.g., lack of HIV training, church-appropriate HIV materials, time, and resources), controversial church issues (e.g., condom use, premarital sex, homophobia), and HIV stigma (18–22)."

    -   Berkley-Patton et al 2016. 
 
::: notes
This is a nice example of a descriptive result where you mention a list of features.

Don't feel that you have to follow these writing guidelines slavishly. Use them if you are not sure how to proceed, but if you are making good writing progress without these guidelines, then ignore them.
:::

## Evaluating, 1 of 3

-   No "bad" or "good" studies
    -   Degrees of evidence
    -   Weak studies better than no studies
-   Questions you can ask
    -   Do the results support the conclusion?
    -   What are the stated limitations?
    -   Can you place this article in "the bigger picture"?

::: notes

When you are evaluating studies, you should comment on the quality of the studies. You may be tempted to create a dichotomy between "good" studies and "bad" studies, but this is a mistake. All studies are good, and we are glad that they get published. 

What you should comment on is how persuasive a study is. By persuasive, I mean persuasive to a clinician. Many articles are very helpful in developing theories, setting research priorities, and so forth, without being far enough along to influence clinical practice.

The strength of the evidence depends on a variety of factors. Keep in mind that no single factor, by itself, is strong enough that its presence makes a study definitive or that its absence makes the study worthless.

One exception is that you sometimes have to draw a line in the sand because of space limitations. If you have three studies and only room to talk about one of them, pick the one that uses blinding, for goodness sakes, if the other two studies are unblinded.

Where the strength of the evidence really comes into play is when you need to reconcile conflicting studies. Report both, if one is much more persuasive than the other, run with that study's conclusion.

Weak evidence is certainly better than no evidence, and you can't draw a firm conclusion that contradicts the evidence, no matter how weak. If the only studies in an area are weak, then consider this your opportunity to cite a gap in knowledge, especially if your proposed research will provide a stronger degree of evidence than what is currently out there.

Some questions that you can ask: Do you feel that the author's interpretation is supported by their data. Did they discussion limitations and are those limitations relevant to you. Have they placed their results in a bigger context?

Don't accept everything blindly just because it has been published. Become critical evaluators. Note that critical means more than just criticizing, though.
:::

## Evaluating, 2 of 3

-   Positive features
    -   Randomization
    -   Blinding
    -   Strong effect
    -   Plausible mechanism

::: notes
I could spend hours and hours talking about evaluating what your book calls "research validity". I prefer talking about how strong or how persuasive the evidence is.. This is the critical appraisal step in Evidence-Based Health Care, and I've written a whole book about it.

Let me just give a quick summary here.

Randomization is a really nice thing to have, because it largely eliminates the problem of alternate explanations like confounding that might otherwise invalidate the research. Randomization is not a panacea, though, and you don't want to become a research bigot who dismisses any research that is not randomized. I'll talk more about randomization next week.

Blinding is hiding information about who gets the drug and who gets the placebo. It's not a serious possibility in many medical studies. I have an off-color joke about this. It actually got published in a medical journal, so don't tell me that the research literature is dry and boring.

Blinding is not possible in some research studies, I say, and then offer up an example of a treatment study where one of the arms is a bilateral orchiectomy. You can't blind that study because sooner or later the patient notices that something is missing. I'll talk a bit more about blinding next week.

A strong effect is more persuasive than a weak effect. How big does an effect have to be to be strong? One commonly cited criteria is an odds ratio larger than 2. This is one of the many false dichotomies in research: an odds ratio of 2.1 is strong and an odds ratio of 1.9 is weak. But it's a start.

The reason that a strong effect is more persuasive is that it is less likely to be produced by biases. Most, but not all, biases are too small to produce an artefactual odds ratio of 2.0 or greater.

If a research finding includes a plausible scientific mechanism that's a good thing.

There's a lot more to evaluation of course. These are just a few highlights.
:::


## Evaluating, 3 of 3
  
-   Negative features
    -   Post hoc changes
    -   High dropout rate
    -   Poor compliance
-   Things not to fuss over
    -   Quality of the journal
    -   Intention to treat analysis

::: notes
I tend to think on the positive side, but there are a few negatives. Post hoc changes in a protocol are usually troublesome. By post hoc, I mean changes that occur in the data collection process after data collection has started or changes in the data analysis plan after some of the data has been analyzed.

A high drop out rate (and anything above 30% is high) is also troublesome.

Poor compliance with the requirements of the research is also a concern. Now please remember that your research subjects are not your slaves and they are always free to do what they think is in their best interests. If your the researcher, just document and report what happened.

There's a lot of discussion of high impact journals, but I have not seen any empirical evidence that the quality of the research is higher in those journals. It may be, or it may be that the difference is in the types of problems studied rather than in the quality of the studies themselves. 

Also, if I can editorialize here, intention to treat analysis, while it is a nice thing, is way overrated. I'll talk about intention to treat analysis next week.
:::
  
