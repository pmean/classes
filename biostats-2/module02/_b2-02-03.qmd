---
title: "Variable selection"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## Avoid needlessly complex regression models

-   "Everything should be made as simple as possible, but not simpler" Albert Einstein (?)
-   "If you have two competing ideas to explain the same phenomenon, you should prefer the simpler one." Occam's Razor
-   "The parsimony principle for a statistical model states that: a simpler model with fewer parameters is favored over more complex models with more parameters, provided the models fit the data similarly well." - ClubVITA

::: notes
Here are a series of quotes that all advise against needlessly complex models. There is some empirical evidence that complex models do not extrapolate well, but these exhortations for simplicity are largely based on subjective opinions. Even so, this is an attitude that I endorse heartily (with one reservation).
:::

## Choosing independent variables is a balancing act

-   Too many is bad
    -   Expensive
    -   May not replicate well
-   Too few is bad
    -   May not explain much of the variation
    -   May miss important features
-   Finding the right balance depends on your goals
    -   Mechanistic approach versus prediction
    
::: notes
I have heard many times that Statistics is as much of an art as it is of a science, and I largely agree with this perspective. The selection of independent variables in a multiple linear regression model is an example of the artistic side of statistics.

It is a balancing act. You want a model that is as simple as possible, but not so simple as to oversimplify. This is a very subjective criteria, but it should not be ignored.

Selecting too many independent variables is definitely bad. It can be expensive. Looking at the data on body fat measurements, do you really need to measure circumferences at 10 different parts of the body to get a good handle on how much body fat someone has? Maybe you could do almost as well with only measurements at 3 locations.

The other problem with overly complex models is that they tend to do a poor job of replication. You might do really well with your own dataset, but when others collect similar data, the overly complex models generally don't look quite as good. You may even find that new data collected at your own location does work so well with a really complex model.

A model that is too simple might not explain enough of the variation in the data. It may also miss out on important features.

Deciding where to go between the simplest models and the most complex models is a tricky choice. It may depend on the goals of your research. If your goal is to try to understand the mechanisms behind a disease or behind a health care delivery process, you might gravitate towards the simpler models. Including too many variables just makes it harder to understand what is going on "under the hood". Too many variables is less of a problem if your main goal is prediction. You can tolerate a few extra unneeded variables. The complexity doesn't hurt you as much if you are only interested in prediction.

I'll return to this topic in the next section.
:::


## Counterpoint on complexity

-   Machine learning algorithms
-   Risk adjustment

::: notes
There are many who advocate that machine learning algorithms, which automatically choose among some very complex models, are preferred in many settings. In particular, if you are interested in prediction but not mechanisms, then complexity can be your friend, as long as you are careful about this.

A second area where a more complex model is called for is in risk adjustment. If you have an observational study and you want to control for confounding, it is often best to adjust for every medically plausible variable that could influence your outcome. A simple model may lead to residual confounding, a lingering bias that remains after you try to adjust with a simple model.
:::

## Rule of 15

-   Developed by Frank Harrell in a different context
    -   Ratio of observations to independent variables
        -    n/k > 15
        -    Some use 10 instead of 15
    -   Smaller ratios imply poor replicability
-   Not a replacement for a power calculation
-   Some researchers have argued against this rule

::: notes
A commonly cited rule about model complexity is the rule of 15. It is a reminder that the more complex your statistical model, the more data you need.

This rule was originally developed by Frank Harrell in a slightly different context, logistic regression. It is commonly applied in a multiple linear regression context, and it seems reasonable enough.

The rule is that the number of observations in your dataset need to be a lot larger than the number of independent variables. Fifteen times larger, as a matter of fact. So if you have 20 independent variables, then you need at least 300 observations to insure a ratio of 15 to 1. If you have 6 independent variables, you need at least 90 observations.

Now there is nothing stopping you from running a regression model with fewer observations. If you have 50 observations and you want to run a model with 10 independent variables (a 5 to 1 ratio), there is nothing to stop you. 

The rule of 15 is a rule of thumb and no one ever got thrown in jail for violating a rule of thumb.

The concern with running a really complex model with a small number of observations is that these models do not replicate well. 

Now some researchers argue that a 10 to 1 ratio of observations to independent variables is fine.

Other researchers have argued that this rule of thumb is not valid and that it is okay to run complex models on small datasets. This is a minority viewpoint, however, and running a 10 variable model on just 50 observations is likely to get you in trouble.

An important point to remember here is that k is the number of candidate variables that are considered at some time during the data analysis. It is NOT the number of variables in the final model.
:::

## Comparing models with k and k-1 predictors

-   $Y_i=\beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+...+\beta_k X_{ki}+\epsilon_i$
    -   Test $H_0:\ \beta_j=0$ versus $H_0:\ \beta_j \ne 0$
        -   Remaining $\beta$'s  could be anything
    -   Use least squares to estimate $\hat\beta_0,\ \hat\beta_1,\ \hat\beta_2,\ ...,\ \hat\beta_k$
    -   Compute $T=\frac{\hat\beta_j}{se(\hat\beta_j)}$
    -   Accept $H_0$ if T is close to zero
    -   Reject $H_0$ if T is large negative or large positive
    
::: notes    
If you want to look at the impact of an individual independent variable in a multiple regression model, compute the full model and compare the coefficient of one of the independent variables to its standard error. If that ratio is close to zero, accept the null hypothesis.

Now keep in mind that the test is highly dependent on what other variables are in the model. You are looking at the impact of the jth variable while holding the other variables constant. That is NOT the same as the impact of the jth variable by itself in a single variable linear regression model.
:::

## Testing the impact of chest circumference

![](images/regression-coefficients-01.png)

::: notes
What is the impact of chest circumference in a model that already includes abdomen and hip circumference? This table shows the regression coefficients and the standard errors for a multiple linear regression model with all three variables. The regression coefficient for chest is -0.186 and the standard error is 0.081. The test statistic is -2.304 which is large negative and the p-value is small. You should reject H0 and conclude that there is a negative relationship between chest circumference and body fat, after holding hip and abdomen circumference constant.
:::

## Change in R-squared

![](images/r-squared-01.png)

![](images/r-squared-02.png)

- $Partial\ R^2=0.700-0.693=0.007$

::: notes
The regression model with chest, abdomen, and hip accounts for 70% of the variation, but a model with just abdomen and hip does almost as well, accounting for 69.3% of the variation. The difference, 0.7% is the amount of additional variation accounted for when you add chest to a model that already included abdomen and hip.

That seems to contradict the earlier finding with the large negative test statistic and the small p-value. But actually, what it is saying is that although there is sufficient statistical evidence to conclude that chest circumference has a real impact, that impact is small. You will see this a lot, especially with datasets with very large sample sizes. You can often achieve statistical significance for an individual variable, but the practical impact can still be negligible.  
:::

## Partial F test

-   Test $H_0:\ \beta_{k+1}=\beta_{k+2}=...=\beta_{k+m}=0$
-   Calculate ANOVA table for k+m independent variables
    -   Note various SS and MS values for the "full" model
-   Recalculate ANOVA table for only k independent variables
    -   Note various SS and MS values for the "reduced" model.
-   Calculate $F=\frac{(SSR_{full}-SSR_{reduced})/m}{SSE_{full}/(n-(k+m+1))}$

::: notes
Although SPSS does not directly test the joint impact of two or more variables, you can run a test with a bit of work. Fit a model with k+m independent variables and print the analysis of variance table. Call this the full model. Then fit a model with only k independent variables. Call this the reduced model. Then see how much difference there is between the sum of squares regression in the full versus reduced model. This is a measure how much stronger the signal becomes when you add m new indpendent variables to a model which already has k independent variables.
:::

## Testing the impact of hips and chest, 1 of 2

![](images/anova-table-02.png)

::: notes
You can place abdomen in the first block of variables. Then place hips and chest in the second block. SPSS will calculate the analysis of variance table for the reduced model (only abdomen) and the full model (abdomen, hips, and chest).
:::

## Testing the impact of hips and chest, 2 of 2

-   $F=\frac{(SSR_{full}-SSR_{reduced})/m}{SSE_{full}/(n-(k+m+1))}$
-   $F=\frac{(10,548.480-9984.086)/2}{4530.537/(252-(1+2+1))}$
-   $F=\frac{282.2}{18.26829}=15.4$

::: notes
The calculations are tedious, but not too difficult. The F ratio is a lot larger than 1, so you would rejuct the null hypothesis and conclude that hip and/or chest circumference measures provide additional predictive power above and beyond the impact of abdomen alone.
:::

