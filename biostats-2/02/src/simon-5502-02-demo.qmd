---
title: "5502 module 02 demonstration program"
format: 
  html:
    embed-resources: true
editor: source
execute: 
  error: true
---

## File details

This program was written by Steve Simon on 2025-01-23 and is placed in the public domain. You can use this program any way you please.

This program reads [fat.csv][ref-fat-csv], a dataset that compares body fat measurements to girth measurements at various body locations. Refer to the [data dictionary][ref-fat-yaml] for a more detailed description.

[ref-fat-csv]: https://github.com/pmean/data/blob/main/files/fat.csv
[ref-fat-yaml]: https://github.com/pmean/data/blob/main/files/fat.yaml

```{r}
#| label: setup
#| message: false
#| warning: false

library(car)
library(glue)
library(tidyverse)
library(broom)

R.version.string
Sys.Date()
```

#### Comment on the code

The [car][ref-car] package provides some helpful functions for various regression models. This an acronym for Companion to Applied Regression, referring to the book, An R Companion to Applied Regression by John Fox and Sanord Weisberg. In this particular program, we use the vif function in car to produce variance inflation factors.

[ref-car]: https://CRAN.R-project.org/package=car 

## Intermediate files

-   fat: original data from fat.csv
-   m0: linear regression with just an intercept
-   m1: linear regression using abdomen to predict fat_b
-   m2: linear regression using chest, abdomen, hip to predict fat_b
-   m3: linear regression using all circumference measures to predict fat_b
-   m4: linear regression using age to predict fat_b
-   m5: linear regression using age and abdomen to predict fat_b
-   rsq_table: listing of r-squared values for various models
-   r2: applying augment to m2
-   r2a: r2 with all columns except .resid removed

# --- Part 1

## Read the data

```{r}
#| label: read

fat <- read_csv(
  file="../data/fat.csv",
  col_names=TRUE,
  col_types="n")
glimpse(fat)
```

#### Comments on the code

There are 19 columns of data, but they are all numeric, so a single "n" for col_types will do the trick.

## Descriptive statistics for fat_b

```{r}
#| label: descriptives-1

fat |>
  summarize(
    fat_mean=mean(fat_b),
    fat_sd=sd(fat_b),
    fat_min=min(fat_b),
    fat_max=max(fat_b))
```

#### Comments on the code

Normally you should use the na.rm argument for mean, sd, etc., but the data dictionary says that missing value codes are not needed for this data.

#### Interpretation of the output

The average patient has 19% body fat. The data has a wide range from 0% (check this value!) to 45%.

Note: while additional descriptive statistics should be calculated for all the important independent variables, we will not show them here in the interest of time and space.

## Correlations

```{r}
#| label: correlations

fat |>
  select(
    fat_b,
    chest,
    abdomen,
    hip) |>
  cor() |>
  round(2)
```

#### Comments on the code

The [cor][ref-cor] function was discussed in Biostats-1. It does help to round the correlations aggressively.

[ref-cor]: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html

#### Interpretation of the output

Not too surprisingly, there are many strong positive correlations. Patients with higher body fat tend to have higher circumferences and a larger circumference measure at one location tends to be associated with larger circumference measures at other locations.

## Scatterplot of chest circumference and body fat

```{r}
#| label: plot-chest

fat |>
  ggplot() +
  aes(x=chest, y=fat_b) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(
    x="Chest circumference (cm)",
    y="Percentage body fat",
    caption="Steve Simon, 2025-01-25, CC0")
```

#### Interpretation of the output

There is a weak positive relationship between chest circumference and percentage body fat.

## Scatterplot of abdomen circumference and body fat

```{r}
#| label: plot-abdomen

fat |>
  ggplot() +
  aes(x=abdomen, y=fat_b) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(
    x="Abdomen circumference (cm)",
    y="Percentage body fat",
    caption="Steve Simon, 2025-01-25, CC0")
```

#### Interpretation of the output

There is a somewhat stronger positive relationship between abdomen circumference and percentage body fat.

## Scatterplot of hip circumference and body fat

```{r}
#| label: plot-hip

fat |>
  ggplot() +
  aes(x=abdomen, y=fat_b) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(
    x="Hip circumference (cm)",
    y="Percentage body fat",
    caption="Steve Simon, 2025-01-25, CC0")
```

#### Interpretation of the output

There is a weak positive relationship between hip circumference and percentage body fat.

## Linear regression model

```{r}
#| label: models-0-to-3

m0 <- lm(
  fat_b ~ 1, 
  data=fat)

m1 <- lm(
  fat_b ~ abdomen,
  data=fat)

m2 <- lm(
  fat_b ~ chest + abdomen + hip,
  data=fat)

m3 <- lm(
  fat_b ~ 
    neck + 
    chest + 
    abdomen + 
    hip +
    thigh +
    knee +
    ankle +
    biceps +
    forearm +
    wrist,
  data=fat)
```

#### Comments on the code

Although it is not strictly needed for your data analysis, the first model, m0, is a null model that has no predictor variables other than an intercept. This is represented by the formula fat_b~1.

The m1 model is a simple linear regression model using one predictor variable, abdomen circumference, to predict percentage body fat. This is represented by the formula fat_b~abdomen

The m2 model uses three predictor variables, chest, abdomen, and hip circumferences. It will help us address the question whether measurements above and below the abdomen produce better predictions than using just the abdomen circumference alone. To include multiple predictor variables, list them with plus signs between the variables.

The m3 model uses all ten circumference measurements. This model is perhaps a bit excessive, but provides an interesting model that adds the circumferences at the extremities (neck, arms and legs) to the middle three circumferences.

## Analysis of variance for a null model

```{r}
#| label: anova-m0

anova(m0)
```

#### Comments on the code

The [anova][ref-anova] function takes the results of a linear regression model and creates an analysis of variance model.

Although you do not need this for a formal analysis, it is instructive to look at the analysis of variance table for a null model. It has only one row which represents unexplained variation. There is no explained variation in a null model. You do not need to specify units here and in later output from the anova function, but they are the square of the units used by the outcome variable.

[ref-anova]: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/anova.lm.html

#### Interpretation of the output

There is a total amount of unexplained variation of roughly 15,000 before any variables are entered into the model.

## Analysis of variance for a one variable model

```{r}
#| label: anova-m1

anova(m0, m1)
```

#### Interpretation of the output

The unexplained variation of the null model, 15,000, can be is reduced to an unexplained variation of 5,000 by a one variable model. The one variable model has explained variation of 10,000.

The F-statistic is large and the p-value is small, indicating that the one variable model provides a statistically significant prediction of the percentage of body fat. You could state equivalently that the one variable model provides a statistically significant reduction in unexplained variation over a null model.

## Analysis of variance for a three variable model

```{r}
#| label: anova-m2

anova(m0, m2)
```

#### Interpretation of the output

The unexplained variation of the null model, 15,000, can be is reduced to an unexplained variation of 4,500 by a three variable model. The three variable model has explained variation of 10,500.

The F-statistic is large and the p-value is small, indicating that the three variable model provides a statistically significant prediction of the percentage of body fat. You could state equivalently that the one variable model provides a statistically significant reduction in unexplained variation over a null model.

## Analysis of variance for a ten variable model

```{r}
#| label: anova-m3

anova(m0, m3)
```

#### Interpretation of the output

The unexplained variation of the null model, 15,000, can be reduced to an unexplained variation of 4,000 by a ten variable model. The ten variable model has explained variation of 11,000.

The F-statistic is large and the p-value is small, indicating that the ten variable model provides a statistically significant prediction of the percentage of body fat. You could state equivalently that the ten variable model provides a statistically significant reduction in unexplained variation over a null model.

## Calcuation of r-squared for each model

```{r}
#| label: rsq

glance(m0) |>
  bind_rows(glance(m1)) |>
  bind_rows(glance(m2)) |>
  bind_rows(glance(m3)) |>
  mutate(regression_model=glue("m{0:3}")) |>
  mutate(r.squared=round(r.squared, 2)) |>
  mutate(deviance=round(deviance)) |>
  mutate(deviance=format(deviance, big.mark=",")) |>
  select(regression_model, r.squared, deviance) -> rsq_table
rsq_table
```

#### Comments on the code

The [glance][ref-glance] function (part of the broom library) provides a range of summary measures for a linear regression model as well as many other statistical models.

We keep the R-squared value and the deviance. The deviance is a general name for what is unexplained variation (SSE) in the context of linear regression.

THe [bind_rows][ref-bind-rows] function (part of the dplyr/tidyverse libraries) helps to put all the results of the individual models into a single tibble.

[ref-glance]: https://broom.tidymodels.org/reference/glance.lm.html
[ref-bind-rows]: https://dplyr.tidyverse.org/reference/bind_rows.html

#### Interpretation of the output

Unexplained variation for the null model is 15,000. A model with abdomen reduces unexplained variation to 5,000 and accounts for 66% of the variation. A model with chest, abdomen, and hips reduces unexplained variation only slightly more, down to 4,500. This model accounts for 70% of the variation. A model with every circumference reduces unexplained variation a tiny bit more, to about 4,000 and accounts for about 74% of the variation.

# --- Part 2

## Tests of individual regression coefficients, one variable model

```{r}
#| label: coefficients-one-variable

tidy(m1) |>
  mutate(
    p.value =
      case_when(
        p.value <  0.001 ~ "< 0.001",
        p.value >= 0.001 ~ as.character(round(p.value, 3))))
```

#### Comments on the code

The [tidy][ref-broom] function (part of the broom library), provides a nice summary of the coefficients in various statistical models in R. This includes a standardization across all these statistical models. The tidy function also takes the list that is the result of most statistical models in R and extracts the necessary information into a tibble, which makes it easier for you to manipulate the results.

One big modification that this provides is the ability to simplify the display of p-values. Very small p-values are listed as < 0.001 and the other p-values are rounded to three decimal places.

[ref-broom]: https://cran.r-project.org/web/packages/broom/vignettes/broom.html

#### Interpretation of the output

## Tests of individual regression coefficients, three variable model

```{r}
#| label: coefficients-three-variable
tidy(m2) |>
  mutate(
    p.value =
      case_when(
        p.value <  0.001 ~ "< 0.001",
        p.value >= 0.001 ~ as.character(round(p.value, 3))))
```

#### Interpretation of the output

## Tests of individual regression coefficients, ten variable model

```{r}
#| label: coefficients-ten-variable

tidy(m3) |>
  mutate(
    p.value =
      case_when(
        p.value <  0.001 ~ "< 0.001",
        p.value >= 0.001 ~ as.character(round(p.value, 3))))
```

## Comparison of one and three variable models

```{r}
#| label: anova-m1-m2

anova(m1, m2)
```

#### Comments on the code

You can compare the improvement of complex model over a simple model by including the simple and complex model as arguments in the anova function. You can put them in either order, but the table looks closer to the common format if you place the simple model as the first argument.

#### Interpretation of the output

There is a statistically significant improvement using chest and hip in addition to abdomen (m2 vs m1), and for using all circumference measures over just chest, abdomen, and hip. 

## Comparison of three and ten variable models

```{r}
#| label: anova-m2-m3

anova(m2, m3)
```

#### Interpretation of the output

There is a statistically significant improvement using all circumference measures over just chest, abdomen, and hip. 

# --- Part 3

## Residuals

```{r}
#| label: augment

r2 <- augment(m2)
```

## Residual plots versus predicted

```{r}
#| label: fitted-plot
r2 |>
  ggplot() +
  aes(x=.fitted, y=.resid) +
  geom_point() +
  labs(
    x="Predicted values from three variable model",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0") 
```

## Residual plots versus non-included variables

```{r}
#| label: thigh-plot

r2 |> 
  select(.resid) |>
  bind_cols(fat) -> r2a

r2a |>
  ggplot() +
  aes(x=thigh, y=.resid) +
  geom_point() +
  labs(
    x="Thigh circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

#### Interpretation of the output

For this and the remaining plots it looks like there is no evidence that these variables might provide additional predictive power above and beyond the prediction using chest, abdomen, and hip. There is an outlier on the high end of most circumference measures that may need to be investigated.

```{r}
#| label: knee-plot
r2a |>
  ggplot() +
  aes(x=knee, y=.resid) +
  geom_point() +
  labs(
    x="Knee circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

```{r}
#| label: ankle-plot

r2a |>
  ggplot() +
  aes(x=ankle, y=.resid) +
  geom_point() +
  labs(
    x="Ankle circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

```{r}
#| label: biceps-plot

r2a |>
  ggplot() +
  aes(x=biceps, y=.resid) +
  geom_point() +
  labs(
    x="Biceps circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

```{r}
#| label: forearm-plot
r2a |>
  ggplot() +
  aes(x=forearm, y=.resid) +
  geom_point() +
  labs(
    x="Forearm circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

```{r}
#| label: wrist-plot
r2a |>
  ggplot() +
  aes(x=wrist, y=.resid) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(
    x="Wrist circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

```{r}
#| label: neck-plot
r2a |>
  ggplot() +
  aes(x=neck, y=.resid) +
  geom_point() +
  labs(
    x="Neck circumference (cm)",
    y="Residuals from three variable model",
    caption="Steve Simon, 2025-01-27, CC0")
```

# --- Part 4

## Variance inflation factor

```{r}
#| label: vif

library(car)
vif(m2)
```

# --- Part 5

## Mediation analysis

## Linear regression model

```{r}
#| label: models-4-to-6

m4 <- lm(
	fat_b ~ age, 
	data=fat)

m5 <- lm(
	abdomen ~ age, 
	data=fat)

m6 <- lm(
	fat_b ~ age + abdomen, 
	data=fat)
```

#### Comments on the code

Mediation is tested in three steps. Does the variable influence the outcome, does the variable influence the mediator, and does the effect of the variable on the outcome disappear when you model both the variable and the mediator in predicting the outcome.


```{r}
#| label: coefficients-m4

tidy(m4) |>
  mutate(
    p.value =
      case_when(
        p.value <  0.001 ~ "< 0.001",
        p.value >= 0.001 ~ as.character(round(p.value, 3))))
```

#### Comments on the output



```{r}
#| label: coefficients-m5

tidy(m5) |>
  mutate(
    p.value =
      case_when(
        p.value <  0.001 ~ "< 0.001",
        p.value >= 0.001 ~ as.character(round(p.value, 3))))
```

#### Comments on the output




```{r}
#| label: coefficients-m6

tidy(m6) |>
  mutate(
    p.value =
      case_when(
        p.value <  0.001 ~ "< 0.001",
        p.value >= 0.001 ~ as.character(round(p.value, 3))))
```

#### Comments on the output



## Save everything

```{r}
#| label: save
save(
  fat,
  m0,
  m1,
  m2,
  m3,
  m4,
  m5,
  m6,
  rsq_table
  r2,
  file="../data/module02.RData")
```