---
title: "Stepwise regression"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## What is a good model

-   "Everything but the kitchen sink" model
-   Stepwise models
-   Hierarchical models
-   "Maximimum adjusted R-squared" model
-   "Out of sample error" model
-   "Use your brain" model
-   Some combination of the above?

::: notes

Finding a good regression model is not easy. There are several criteria that have been used.

:::

## "Everything but the kitchen sink" model

-   Include anything that seems remotely plausible
-   Advantages
    -   Simple computationally
-   Disadvantages
    -   Does not identify mechanisms
    -   Can be expensive
    -   Fails if k is very large
    
::: notes

"Everything but the kitchen sink" is an American idiom for "everything imaginable". In this approach, you brainstorm a list of variables that might be associated with your outcome. Use your knowledge of past research in the area, your understanding of the medical and scientific processes involved, and just your plain old intuition. Anything that seems remotely possible to be related to the outcome variable is included in the regression model.

:::

## Kitchen sink model for body fat

```{r}
#| label: setup-4
#| message: false
#| warning: false

library(broom)
library(glue)
library(tidyverse)
load("../data/module02.RData")
```

```{r}
#| label: anova3

anova(m0, m3)
```


::: notes

Here's a model with all the circumference measurements included.

:::

## Stepwise model, 1 of 2

-   Forward selection
-   Backward elimination
-   Composite of both

::: notes

A stepwise model uses p-values to identify which variables to add to the model (forward selection) or which variables to remove from the model (backward elimination). You can combine these two approaches.

Forward selection starts with nothing. It looks at the p-values for every single variable model and chooses the variable with the smallest p-value. Then it looks at every possible two variable model that includes the first selected variable and chooses the second variable based on the smallest p-value. Then it looks at every possible third variable in a model that includes the first two variables. And so forth.

Backward elimination starts with everything. It looks at all the p-values for the k independent variables in the model and eliminates the variable with the largest p-value. Then it looks at all the p-values in the model with k-1 independent variables and removes the independent variable with the smallest p-value.

Both approaches take a "one step at a time" approach. You might be tempted in the backward elimination step to toss out every independent variable with a large p-value. But the p-values do change as you start removing variables. One of your independent variables might have a large p-value when you have all k variables in the model, but that p-value might shrink as other variables are tossed out.

A general approach is to start with a forward selection approach, but each time a new variable is added, you check to see if one of the variables which had a promising p-value earlier now looks not so hot. So each forward step is followed with a backward elimination check.

:::

## Stepwise model, 2 of 2
-   Advantages
    -   Automated in SPSS and other packages
-   Disadvantages
    -   Does not incorporate medical knowledge
    -   Does not control Type I error rate

::: notes

The forward selection and backward elimination approaches are easy to program and are available in SPSS and most other statistical software packages.

Stepwise regression, however, does not incorporate medical knowledge into the process. You might have two variables that both have large p-values in a backward elimination approach. One is well known to be important by anyone with a medical background, but if it's p-value is a bit larger than the second one that is a bit of a stretch, you will make the wrong choice, at least from a medical perspective.

The other criticism of stepwise models is that they do not control the Type I error rate. You use repeated hypothesis tests and the chances of making a Type I error and including a variable that does not really belong gets inflated. The number of tests that stepwise regression requires is very large, even if you only have a have only a handful of variables.

:::

## Hierarchical models

-   Sometimes variables fall into natural groups.
    -   Demographic features
    -   Patient frailty
    -   Environmental factors
    -   Current treatments
-   Enter data in blocks

::: notes

Be careful with the term "hierarchical" as it means many things. In the context of multiple linear regression, it means that variables fall into natural groups or blocks. Some examples are variables that represent demographic features (such as age, race, and gender) versus variables that represent patient frailty (stage of cancer, severity of illness, previous history of health issues) versus environmental factors (exposure to allergens, second hand smoke) versus current treatments (number of medications, dosages).

In a hierarchical multiple linear regression model, you enter all variables in a particular group together at once and then add a second group, third group, etc.

:::

## "Maximimum adjusted R-squared" model

-   Fit all possible models ($=2^k$)
    -    Select model with largest adjusted R-squared
    -    Alternative criteria
         -    Mallows Cp
         -    AIC, AICc
         -    LASSO

::: notes

If the number of independent variables is not too big, you can look at every possible combination of independent variables. For the body fat example, there are ten measures and this means examining 1,024 different models. Not easy for you maybe, but your computer can do this in the blink of an eye. 

Be careful, though. You may notice that $2^k$, the number of models, increases (literally) exponentially. With 20 independent variables, you would need to fit over a million models. With 30, it would be over a billion.

You cannot choose the model that maximizes R-squared. R-squared never decreases as the model becomes more complex. It may only increase by a trivial amount, but because it never decreases. Choosing the model with the largest R-squared value will always select the most complex model, the one with every independent variable.

Adjusted R-squared can go down, because it adjusts for the degrees of freedom. In general, though, adjusted R-squared doesn't do enough adjusting and will tend to select needlessly complex models. There are other criteria: Mallows Cp, Akaike Information Criteria (AIC), a corrected version of AIC (AICc), and LASSO. These all do a better job of avoiding overly complex models.

:::

## "Out of sample error" model

-   Split data into training/test sets
    -    Use training data to build the model
    -    Use test dataset to evaluate fit

## "Use your brain" model

-   Previous research
-   Knowledge of medicine/science
-   Superior to "brainless" approaches
    -   “Ginny!" said Mr. Weasley, flabbergasted. "Haven't I taught you anything? What have I always told you? Never trust anything that can think for itself if you can't see where it keeps its brain?” from Harry Potter and the Chamber of Secrets, J.K. Rowling.

::: notes
There is a tendency to forget that you yourself have a lot to contribute to the process of selecting the right independent variables. You are familiar with previous research and you have insights into the medicine and science that underlies the model you are trying to build.

Relying solely on automated procedures is often a mistake because the automated procedures do not use anything beyond the numbers themselves.

If there is a problem with these approaches and more generally with many machine learning models, it is highlighted by the quote from J.K. Rowling. She was not thinking about machine learning when she crafted this quote, but it certainly applies.

:::

## Use a mixture

-   Use a mixture of science/medicine with automated approaches?
    -   Story about an industrial process
    
::: notes

At a minimum, use your knowledge as you evaluate these models. Avoid the tendency to think of a stepwise approach or any other automated approach as the final word. Think of it as an intelligent assistant, who might save you time, but you still have to carefully check everything they produce.

There's an interesting anecdote that I heard many years ago (so long ago that I can't remember the source). A researcher was examining an industrial process and was rating the importance of various factors in controlling the output. The statistician ranked all the variables using some sort of stepwise algorithm. They listed the most important variable, second most important, etc. and conclude with "and the least important variable is the amount of water present."

At the point the whole audience erupted in laughter. They knew that water was very important because even a small amount of water in the system would cause a terrible explosion. In fact, the factory made every attempt to tightly control the amount of water. This control led to a very small standard deviation and very little power to detect a significant effect. The moral is to never blindly trust a model built solely on the data with no consideration of previous research and the underlying scientific and medical mechanisms.

:::