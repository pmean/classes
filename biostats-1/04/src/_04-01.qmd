---
title: "04-01, Scatterplots and correlations"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## Assessing relationships (bivariate statistics), 1

-   Between two continuous variables
    -   Scatterplot
    -   Correlation

:::notes
The previous modules talked about how to examine individual variables. I used the term univariate statistics. After you understand how each individual variable behaves, you need to start examining how variables relate to one another. I will focus on graphical approaches, but also some numeric measures.

Recall that continuous variables have a large number of possible values, potentially any value in some interval. Categorical variables have a small number of possible values with each value corresponding to a particular label.

If you are visualizing the relationship between two continuous variable, use a scatterplot. The best numeric summary is a correlation coefficient.
:::

## Assessing relationships (bivariate statistics), 2


-   Between a categorical and a continuous variable
    -   Boxplot
    -   Effect size

    
:::notes
If you are visualizing the relationship between a categorical and a continuous variable, use a boxplot. A common numeric summary is the effect size. I do not like effect sizes, but they are used so commonly that I have to teach them.

:::

## Assessing relationships (bivariate statistics), 3

-   Between two categorical variables
    -   Bar plots
    -   Many numeric measures (covered later)
    
:::notes

If you are visualizing the relationship between two categorical variables, use a bar plot of the probabilities. There are many numeric measures, and I will defer discussion of these until a later module.
:::

## Scatterplot

-   Assess relationship between two continuous variables
    -   Outcome on y-axis
    -   Exposure/intervention on x-axis
    
:::notes
The scatterplot is a simple and easy way to examine the relationship between two independent variables. By tradition (and for no other good reason), the outcome variable is plotted on the y-axis (the vertical axis) and the exposure or treatment variable is plotted on the x-axis (the horizontal axis). If you conceive of the relationship as cause and effect, the cause variable goes on the x-axis Sometimes it is not clear which is which. You have to use your best judgement. I will usually provide some guidance in the programming assignments.
:::
  
## Patterns on a scatterplot

-   Strong, weak, or no relationship
-   Direction of relationship
-   Nonlinear patterns
    -   Diminishing returns
    -   Exponential acceleration
    -   Hormesis
    
:::notes
Scatterplots require you to make a subjective interpretation. Is the relationship strong or weak? Is there any relationship at all?

What is the direction of the relationship? Does an increase in exposure lead to an increase in the outcome? That's a positive relationship. If an increase in exposure leads to a decrease in the outcome, that's a negative relationship.

Look for nonlinear patterns. These can take many forms, but three common ones are diminishing returns, exponential acceleration, or hormesis. I'll show examples of these in a few minutes.
:::

## Covariance

-   $Cov(X, Y) = \frac{\Sigma (X_i-\bar{X})(Y_i-\bar{Y})}{n-1}$
    -   $X_i > \bar{X}$ and $Y_i > \bar{Y};\ \ \ $ (+) $\times$ (+) = +
    -   $X_i < \bar{X}$ and $Y_i < \bar{Y};\ \ \ $ (-) $\times$ (-) = +
    -   $X_i > \bar{X}$ and $Y_i < \bar{Y};\ \ \ $ (+) $\times$ (-) = -
    -   $X_i < \bar{X}$ and $Y_i > \bar{Y};\ \ \ $ (-) $\times$ (+) = -
    
:::notes
Your book introduces the term covariance first before defining correlation. The covariance is a product involving the individual X and Y values compared to their respective means.

There are four possibilities. An individual X_i and Y_i values could both be above average. This makes the terms (X_i-Xbar) and (Y_i - Ybar) both positive. When you multiply two positive numbers together, you get a positive result.

A second possibility is that both individual values are below average. This makes the terms (X_i-Xbar) and (Y_i - Ybar) both negative. When you multiply two negative numbers you get a positive result.

If most of the data follows this pattern, above average values of X associated with above average values of Y and below average values of X associated with below average values of Y, then the covariance will be positive.

You might have the opposite pattern occur. Above average values of X are mostly associated with below average values of Y and vice versa. In this case, the covariance will be negative.
:::
    
## The correlation coefficient, 1

-   $r = Corr(X, Y) = \frac{Cov(X, Y)}{S_X S_y}$
    -   r is always between -1 and +1.
    -   r is a unitless quantity

:::notes
The correlation coefficient between X and Y is computed as the covariance between X and Y divided by the standard deviation of X and the standard deviation of Y. 

This creates a unitless quantity. While the covariance will change when you convert measurements from grams to kilograms, the correlation will not.

In general, I am not in favor of unitless quantities. They may allow you to compare different outcomes, but to understand the practical implications, you need to use a measure with units. I'll address this a bit later with the discussion about effect sizes. 
:::
        
## The correlation coefficient, 2

-   Informal interpretation
    -  +0.7 < r < +1.0 is a strong positive relationship
    -  +0.3 < r < +0.7 is a weak positive relationship
    -  -0.3 < r < +0.3 is little or no relationship
    -  -0.7 < r < -0.3 is a weak negative relationship
    -  -1.0 < r < -0.7 is a strong negative relationship

:::notes
The closer the correlation is to +1, the stronger the positive relationship. The closer the correlation is to -1, the stronger the negative relationship. A correlation close to 0 implies little or no relationship.

There is some debate about the dividing lines between strong and weak relationships, but a dividing line around +/-0.6 or 0.7 is common. Anything closer to zero than +/-0.2 or 0.3 is considered evidence of little or no relationship.
:::

## An example of a strong positive relationship

```{r strong-positive}
#| echo: false

library(MASS)
library(tidyverse)
# set.seed(123) # Set a seed for reproducibility
illustrate_correlation <- function(rho) {
  bvn <- mvrnorm(
    n = 100, 
    mu = c(0, 0), 
    Sigma = matrix(
  	  c(1.0, rho, rho, 1.0), 
      ncol = 2))
  x <- bvn[, 1]*10+50
  y <- bvn[, 2]*3+20
  r <- cor(bvn)
  data.frame(x, y) |>
	  ggplot(aes(x, y)) + 
	    geom_point() +
	    ggtitle(paste0("r = ", round(cor(x, y), 2))) +
  	  expand_limits(
  	  	x=c(20, 80),
  	  	y=c(10, 30))
}
illustrate_correlation(0.8)
```

## An example of a weak positive relationship

```{r weak-positive}
illustrate_correlation(0.5)
```

## An example of a little or no relationship

```{r no-relationship}
illustrate_correlation(0.1)
```

## An example of a weak negative relationship

```{r weak-negative}
illustrate_correlation(-0.5)
```

## An example of a strong negative relationship

```{r strong-negative}
illustrate_correlation(-0.8)
```
