---
title: "Comments for MEDB 5501, Week 12"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## Nonparametric tests

-   Most developed in the 1950s.
-   A test that does not rely on distributional assumptions
    -   "Distribution free test" is more mathematically precise
-   Important!! Other assumptions will remain
    -   Especially independence
-   Nonparametric implies no population parameter
    -   Mathematical form of hypotheses is tricky

::: notes
There are a broad range of statistical tests that are described as non-parametric tests. These were almost all developed in the 1950s. Statistics, as a discipline, was relatively new at this time. Sad to say, but relatively little has been done since the 1950s on these tests. In general, they do not extend easily to more complex settings like risk adjustment models.
:::

## The probability basis for ranking

You randomly draw three balls from an urn containing balls numbered 1 through 9. What are the chances that you draw the highest three balls (7, 8, and 9)?

![](../images/nine-ball.jpg)

::: notes
Add note.
:::

## List all the possible outcomes.

```{r}
n <- 9
m <- n*(n-1)*(n-2)/6
x <- rep(NA, m)
y <- rep(NA, m)
r <- 1:n
u <- 0
for (i in 1:(n-2)) {
  for (j in (i+1):(n-1)) {
    for (k in (j+1):n) {
      u <- u+1
      x[u] <- paste(r[i], r[j], r[k], sep=",")
      y[u] <- i+j+k
    }
  }
}
for(u in 0:11) {
  cat(x[(7*u+1):(7*u+7)], sep="  ")
  cat("\n")
}
```

::: notes
It's more than a bit tedious, but you can do it if you approach the problem systematically. There are 84 total choices, so the probability is 1/84.
:::

## Artificial data set

```{r, eval=FALSE}
a <- round(rgamma(9, 0.2, 0.00001))
b <- sort(a)[7]
x1 <- a[a >= b]
x2 <- a[a <  b]
artificial_data <- 
  data.frame(
    group=rep(LETTERS[24:25], c(3,6)), 
    value=c(x1, x2))
write.csv(artificial_data, "../data/data-12-artificial.csv")
```

```{r}
artificial_data <- read.csv("../data/data-12-artificial.csv")
print(artificial_data, row.names=FALSE)
```

::: notes
Add note.
:::

## Test statistic, $\Sigma R(X_i)$, 1 of 2

```{r}
for (i in 6:15) {
  cat(sprintf("%2i", i), x[y==i], sep="  ")
  cat("\n")
}
```

::: notes
Add note.
:::

## Test statistic, $\Sigma R(X_i)$, 2 of 2

```{r}
for (i in 16:24) {
  cat(sprintf("%2i", i), x[y==i], sep="  ")
  cat("\n")
}
```

::: notes
Add note.
:::

## Alternate calculation, $P[X>Y]$

```{=tex}
\begin{matrix} & X_{1} & X_{2} & X_{3} \\ Y_{1} & 0\ or\ 1 & 0\ or\ 1 & 0\ or\ 1 \\ Y_{2} & 0\ or\ 1 & 0\ or\ 1 & 0\ or\ 1 \\ Y_{3} & 0\ or\ 1 & 0\ or\ 1 & 0\ or\ 1 \\ Y_{4} & 0\ or\ 1 & 0\ or\ 1 & 0\ or\ 1 \\ Y_{5} & 0\ or\ 1 & 0\ or\ 1 & 0\ or\ 1 \\ Y_{6} & 0\ or\ 1 & 0\ or\ 1 & 0\ or\ 1 \end{matrix}
```
::: notes
Add note.
:::

## Break #1

-   What have you learned
    -   The probability basis for many nonparametric tests
-   What is coming next
    -   A formal description of the Mann-Whitney-Wilcoxon test

## Mann-Whitney-Wilcoxon (MWW) test, 1 of 2

-   Also known as Wilcoxon-Mann-Whitney test
-   Two independent samples
    -   $X_1,X_2,...,X_n$
    -   $Y_1,Y_2,...,Y_m$
    -   n, m do not have to be equal

::: notes
Add note.
:::

## Mann-Whitney-Wilcoxon (MWW) test, 2 of 2

-   $U = \Sigma_i R(X_i)$
    -   Accept H0 if U is close to $n \times\frac{n+m+1}{2}$
-   $W = \Sigma_i \Sigma_j H[X_i-Y_j]$
    -   H is a counting function
        -   = 1 for zero or positive values
        -   = 0 for zero or negative values\
    -   Accept H0 if W is close to $\frac{nm}{2}$

::: notes
Add note.
:::

## What hypothesis are you testing

-   Tricky because non-parametric means no parameter
    -   $H_0:\ P[X>Y] = 0.5$ or
    -   $H_0:\ X_{0.5}=Y_{0.5}$ (controversial)
-   Alternative formulation: shifted distributions
    -   $P[X \le t]=P[Y - \Delta \le t]$
    -   $H_0:\ \Delta=0$

::: notes
Add note.
:::

## Assumptions of the MWW test

-   Observations are independent
    -   Within each sample
    -   Between samples
-   Distributions identical when $H_0$ is true, or
-   Distribution of Y is uniformly shifted from the distribution of X
    -   Unequal variances is a violation of either of these assumptions

::: notes
Add note.
:::

## How to handle ties, 1 of 4

```{r}
r <- rank(artificial_data$value)
artificial_data$value[r %in% 5:7] <-
  artificial_data$value[r==6]
artificial_data$rank <-
  as.character(rank(artificial_data$value))
artificial_data$rank[r %in% 5:7] <- "?"
artificial_data
```

::: notes
Add note.
:::

## How to handle ties, 2 of 4

```{r}
r <- rank(artificial_data$value)
artificial_data$value[r %in% 5:7] <-
  artificial_data$value[r==6]
artificial_data$rank <-
  as.character(rank(artificial_data$value))
artificial_data$rank[r %in% 5:7] <- "6"
artificial_data
```

::: notes
Add note.
:::

## How to handle ties, 3 of 4

```{r}
n <- 9
m <- n*(n-1)*(n-2)/6
x <- rep(NA, m)
y <- rep(NA, m)
r <- c(1:4, rep(6,3), 8:9)
u <- 0
for (i in 1:(n-2)) {
  for (j in (i+1):(n-1)) {
    for (k in (j+1):n) {
      u <- u+1
      x[u] <- paste(r[i], r[j], r[k], sep=",")
      y[u] <- r[i]+r[j]+r[k]
    }
  }
}
for (i in 6:15) {
  cat(sprintf("%2i", i), x[y==i], sep="  ")
  cat("\n")
}
```

::: notes
Add note.
:::

## How to handle ties, 4 of 4

```{r}
for (i in 16:24) {
  cat(sprintf("%2i", i), x[y==i], sep="  ")
  cat("\n")
}
```

::: notes
Add note.
:::

## Break #2

-   What have you learned
    -   A formal description of the Mann-Whitney-Wilcoxon test
-   What is coming next
    -   Tables, exact tests, Monte Carlo tests, and normal approximation

## MWW test using tables

![](../images/mww-04.png) ::: notes

:::

## MWW test using an exact test

```         
# r code to list all possible test values for
# MWW test with n=3, m=6

n <- 3
m <- 6
T <- rep(NA, (n+m)*(n+m-1)*(n+m-2)/6)
r <- 1:(n+m)
u <- 0

for (i in 1:(n+m-2)) {
  for (j in (i+1):(n+m-1)) {
    for (k in (j+1):(n+m)) {
      u <- u+1
      T[u] <- r[i]+r[j]+r[k]
    }
  }
}
```

::: notes
Add note.
:::

## MWW test using a Monte Carlo approximation

```         
# r code to compute a Monte Carlo approximation 
# for the Mann-Whitney-Wilcoxon test with n=3, m=6

n <- 3
m <- 6
reps <- 10000
T <- rep(NA, reps)
r <- 1:N
u <- 0

for (i in 1:reps) {
  T[i] <- sum(sample(r, 3))
}
```

::: notes
Add note.
:::

## MWW test using a normal approximation

```{r}
library(ggplot2)
ggplot(data.frame(T=y), aes(x=T)) +
  geom_histogram(
    binwidth=1, 
    color="black", 
    fill="white")
```

::: notes
Add note.
:::

## When should you use MWW test?

-   Small sample sizes and/or
-   Non-normality and/or
-   Ordinal outcome variable

::: notes
There is a fair amount of disagreement about when to use the MMW test. Typically some combination of small sample size, non-normality, or ordinal data will come into play.
:::

## Which MMW test?

-   Tables: never!
-   Exact test: small (n+m \< 100) sample sizes
-   Monte Carlo approximation: moderate (100 \< n+m \< 1 million) sample sizes
-   Normal approximation: moderate or large (n+m \> 1 million) sample sizes
    -   The cutoffs for small, moderate, and large are my interpretations only.

::: notes
I would not recommend using tables. They are tedious, error prone, and only work if there are no ties in the data.

An exact test is ideal. It does require a lot of computing power, but today's computers can handle exact tests for a total sample size of a hundred or so.

The Monte Carlo approximation is a fall back option for moderate sample sizes. This depends on how much computing power
:::

## Example: FEV data, 1 of 2

![](../images/mww-05.png)

::: notes
This is a dataset you've seen before, forced expiratory volume in a sample of children. You have information on age, height, sex, and smoking status.

Although the sample size is large and the data appears to be not too far from a normal distribution, let's run a MWW test comparing FEV values for males and females.

The average rank is higher for males than females. It is only slightly higher, but the sample size is fairly large.
:::

## Example: FEV data, 2 of 2

![](../images/mww-06.png)

::: notes
You cannot get an exact test with this sample size, but the normal approximation should suffice. The p-value is small, so you should reject the null hypothesis and conclude that males tend to have larger FEV values than females.
:::

## Common nonparametric tests

-   Mann-Whitney-Wilcoxon vs. independent samples t-test
-   Kruskal-Wallis test vs. ANOVA
-   Wilcoxon signed rank test vs. paired t-test
-   Friedman test vs. randomized block design

Note: in this class, homework, quizzes, and final exam will only cover the MWW test.

::: notes
You've already seen a lot about the MWW test. It is an alternative to the independent samples t-test that you could consider if you are worried about the normality assumption.

An extension to more groups is the Kruskal-Wallis test. It is identical to the MWW test when you have two groups, and can handle a hypothesis involving three or more groups. It is an alternative to ANOVA that you should consider if you are worried about the normality assumption.

The Wilcoxon signed rank test works with paired data and is an alternative to the paired t-test.

Finally, the Friedman test is an alternative to ANOVA with a randomized block design.

Although your book and the lecture videos by Monica Gaddis cover all four tests, I want to give you a small break. You need to be familiar with the MWW test for homework, quizzes, and the final exam, but you will not see anything related to the other three nonparametric tests.
:::

## Criticisms of nonparametric tests

-   Less power than parametric tests
    -   Only a small loss in power
    -   Only when normality assumption is satisfied
-   Difficult to produce confidence intervals
-   Difficult to perform risk adjustments
-   Limited extensions to more complex settings

::: notes
Add note.
:::

## Criticisms of parametric tests

-   Perform poorly when data skewed, or has outliers
-   Cannot handle ordinal data

::: notes
Add note.
:::

## Decision algorithm

-   Decide prior to data collection
-   Review assumptions and then decide

::: notes
Add note.
:::
