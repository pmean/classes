---
title: "05-03, The Least Squares principle"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## The population model

-   $Y_i=\beta_0+\beta_1 X_i + \epsilon_i,\ i=1,...,N$
    -   $\epsilon_i$ is an unknown random variable
        -    Mean 0, standard deviation, $\sigma$
        -    Often assumed to be normal
    -   $\beta_0$ and $\beta_1$ are unknown parameters
    -   $b_0$ and $b_1$ are estimates from the sample
    
::: notes
Add note.
:::

## Least squares principle, 1

-   Collect a sample
    -   $(X_1,Y_1),\ (X_2,Y_2),\ ...\ (X_n,Y_n)$
-   Compute residuals
    -   $e_i=Y_i-(b_0+b_1*X_i)$
    -   Choose b_0 and b_1 to minimize $\Sigma e_i^2$

## Least squares principle, 2

```{r setup-2}
library(tidyverse)
load("../data/module05.RData")
```

```{r draw-hypothetical}
draw_line(20, 1)
```

:::notes
Here's one possible choice for the regression line, an intercept of 20 and a slope of 1. It does an okay job for the leftmost two datapoints, but really overpredicts for the rest of the data.
:::

## Least squares principle, 3

```{r}
draw_line(10, 1)
```

:::notes
Here's an improvement, using an intercept of 10 instead of 20. You can see that there might still be some room for improvement, if we could get the line a bit closer to the first and the fifth data points.
:::

## Least squares principle, 4

```{r}
draw_line(30, -1)
```

:::notes
Twist the line so the slope is -1 instead of +1 and shift the intercept all the way up to 30. This does even better than the first two lines.

It turns out that you can't do any better than this. You might be the line to be a bit closer to one of the data points, but you'd do a lot worse across all of the other data points.
:::

## Least squares principle, 5

-   General solution
    -   $b_1=\frac{\Sigma(X_i-\bar{X})(Y_i-\bar{Y})}{\Sigma(X_i-\bar{X})^2}$
    -   $b_0=\bar{Y}-b_1\bar{X}$
-   Notice the similarity between $b_1$ and r
    -   $b_1=r\frac{S_Y}{S_X}$
    
## Relationship to the correlation coefficient

-   Recall from the previous module
    -   $Cov(X,Y)=\frac{1}{n-1}\Sigma(X_i-\bar{X})(Y_i-\bar{Y})$
    -   $r_{XY}=\frac{Cov(X,Y)}{S_X S_Y}$
-   This implies that
    -   $b_1=r_{XY}\frac{S_Y}{S_X}$

## Important implications

-   $r_{XY}$ is unitless, $b_1$ is Y units per X units
-   $r_{XY}>0$ implies $b_1>0$
-   $r_{XY}=0$ implies $b_1=0$
-   $r_{XY}<0$ implies $b_1<0$
    -   and vice versa

