---
title: "05-03, The analysis of variance table"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

```{r setup-3}
#| message: false
#| warning: false
library(broom)
library(tidyverse)
load("../data/module05.RData")
```


## Sum of squares regression

```{r}
yhat <- 30-simple$x
draw_line(20, 0, pts=yhat)
```

## Sum of squares error

```{r}
draw_line(30, -1)

```

## Sum of squares total / corrected total

```{r}
draw_line(20, 0)
```

## Sum of squares total (uncorrected)

```{r}
draw_line(0, 0, pts=simple$y)
```

## ANOVA table for linear regression

```{=tex}
\begin{matrix}
            &  SS &  df &                  MS &           F-ratio \\
 Regression & SSR &   1 &   MSR=\frac{SSR}{1} & F=\frac{MSR}{MSE} \\
 Error      & SSE & n-2 & MSE=\frac{SSE}{n-2} &                   \\
 Total      & SST & n-1 &                     &
\end{matrix}
```

## Analysis of variance table in R

```{r anova}
anova(m1)
```

:::notes
The analysis of variance table looks different in different software programs and it differs from publication to publication. Notice for example that R does not include a row for total. SPSS, in contrast includes both a corrected total and an uncorrected total.
:::

## R-squared

-   SST, total variation, is split into
    -    SSR, explained variation, and
    -    SSE, unexplained variation
-   $R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$
    -    $0 < R^2 < 1$
    -    Proportion of explained variation
         -   $R^2 > 0.5$ strong
         -   $0.1 < R^2 < 0.5$ weak
-   $R^2 = r^2$


:::notes
You can compare the three sources of variation. SSR is explained variation, variation in the predicted values. SSE is unexplained variation. These add up to total variation. Either explained variation divided by total variation or 1 minus unexplained variation divided by total variation is a measure of the strength of the relationship. If you can explain half of the variation (R2=0.5), that's evidence of a strong relationship. Anything between 10% and 50% is evidence of a weak relationship.

There is a simple mathematical relationship. Square the correlation between X and Y to get R-squared.
:::


## R-squared calculation in R

```{r r-squared}
glance(m1)$r.squared
```

:::notes
Here is the value of R-squared for the breast feeding example. This is a very weak relationship.
:::

## Confidence intervals, 1

-   Population model
    -   $Y_i=\beta_0+\beta_1 X_i + \epsilon_i,\ i=1,...,N$
-   Sample estimates
    -   $b_1=\frac{\Sigma(X_i-\bar{X})(Y_i-]bar{Y)}{\Sigma(X_i-\bar{X})^2}$
    -   $b_0=\bar{Y}-b_1\bar{X}$
   
## Confidence intervals, 2

-   Standard error (se)
    -   $se(b_1)=\sqrt{\frac{MSE}{(n-1)*S_x^2}}$
-   Confidence interval for $\beta_1$
    -   $b_1 \pm t se(b_1)$
    
## Confidence intervals, 3

```{r confidence-interval}
confint(m1)
```

## Hypothesis test, 1

-   $H_0:\ \beta_1=0$
-   $H_1:\ \beta_1 \ne 0$
    -   Accept $H_0$ if $T=\frac{b_1}{se(b_1)}$ is close to zero
    -   $\ $ or Accept $H_0$ if the confidence interval includes zero
    -   $\ $ or Accept $H_0$ if the p-value is large
    
## Hypothesis test, 2

```{r hypothesis-test}
tidy(m1)
```