---
title: "05-03, The analysis of variance table"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

```{r setup-3}
#| message: false
#| warning: false
library(broom)
library(tidyverse)
load("../data/module05.RData")
```


## Sum of squares regression

```{r}
yhat <- 30-simple$x
draw_line(20, 0, pts=yhat)
```

## Sum of squares error

```{r}
draw_line(30, -1)

```

## Sum of squares total / corrected total

```{r}
draw_line(20, 0)
```

## Sum of squares total (uncorrected)

```{r}
draw_line(0, 0, pts=simple$y)
```

## ANOVA table for linear regression

```{=tex}
\begin{matrix}
            &  SS &  df &                  MS &           F-ratio \\
 Regression & SSR &   1 &   MSR=\frac{SSR}{1} & F=\frac{MSR}{MSE} \\
 Error      & SSE & n-2 & MSE=\frac{SSE}{n-2} &                   \\
 Total      & SST & n-1 &                     &
\end{matrix}
```

## Analysis of variance table in R

```{r anova}
anova(m1)
```

:::notes
The analysis of variance table looks different in different software programs and it differs from publication to publication. Notice for example that R does not include a row for total. SPSS, in contrast includes both a corrected total and an uncorrected total.
:::

## R-squared

-   SST, total variation, is split into
    -    SSR, explained variation, and
    -    SSE, unexplained variation
-   $R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$
    -    $0 < R^2 < 1$
    -    Proportion of explained variation
         -   $R^2 > 0.5$ strong
         -   $0.1 < R^2 < 0.5$ weak
-   $R^2 = r^2$


:::notes
You can compare the three sources of variation. SSR is explained variation, variation in the predicted values. SSE is unexplained variation. These add up to total variation. Either explained variation divided by total variation or 1 minus unexplained variation divided by total variation is a measure of the strength of the relationship. If you can explain half of the variation (R2=0.5), that's evidence of a strong relationship. Anything between 10% and 50% is evidence of a weak relationship.

There is a simple mathematical relationship. Square the correlation between X and Y to get R-squared.
:::


## R-squared calculation in R

```{r r-squared}
glance(m1)$r.squared
```

:::notes
Here is the value of R-squared for the breast feeding example. This is a very weak relationship.
:::

## F-ratio, 1

-   $F = \frac{MSR}{MSE}$

:::notes
The F-ratio, MSR divided by MSE. If this value is close to 1, you would accept the null hypothesis. The signal (MSR) is comparable to the noise (MSE). If you see a large positive ratio, that implies that the signal is much stronger than the noise. A large positive ratio would cause you to reject the null hypothesis.

Alternately look at the p-value. If the p-value is large, you should accept the null hypothesis.

I'll talk a lot more about p-values in several of the upcoming modules.
:::