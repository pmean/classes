---
title: "01-05, Hypothesis tests"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## What is a population?

-   Population: a group that you wish to generalize your research results to. It is defined in terms of
    -   Demography,
    -   Geography,
    -   Occupation,
    -   Time,
    -   Care requirements,
    -   Diagnosis,
    -   Or some combination of the above.

::: notes
*Speaker notes*

A population is a group that you have an interest in. You want to get a better understanding of this group, so you conduct a research study and wish to generalize the results of that study to the population.

In clinical research, a population is almost always a group of people. There are a few exceptions. Sometimes you want to characterize inanimate objects, such as a group of hospitals or a group of medical devices. But let's keep the focus on people for now.

A population of people is defined in terms of certain characteristics. Usually it is a combination of these characteristics.
:::

## Example of a population

All infants born in the state of Missouri during the 1995 calendar year who have one or more visits to the Emergency room during their first year of life.

::: notes
*Speaker notes*

Here is an example of a population. It has many of the characteristics described on the previous slide: demography (infants), geography (born in Missouri), time (born in calendar year 1995, during first year of life) and care requirements (one or more ER visits).

Most times the population is so large that it is difficult to get data on all the individuals of that population.

Here, we actually did have access to the data on all 29,637 infants, but most times you would not be so fortunate.
:::

## What is a sample?

-   Sample: subset of a population.
-   Random sample: every person has the same probability of being in the sample.
-   Biased sample: Some people have a decreased probability of being in the sample.
    -   Always ask "who was left out?"

::: notes
*Speaker notes*

A sample is a subset of a population. Because that population of infants was so large, you decided to collect data on a smaller group, a sample of 100 infants, say.

Statistics, according to one definition is the use of data from samples to make inferences about populations. That may be a bit too narrow a definition, but it does characterize quite a bit of what we statisticians do.

A random sample is a special type of sample. It is chosen in a way to insure that every person in the sample has the same probability of being in the sample.

In contrast a biased sample is one where some people in the population have a decreased chance of being in the sample. Often in a biased sample some people in the population are totally excluded.
:::

## An example of a biased sample

-   A researcher wants to characterize **illicit drug use in teenagers.** She distributes a questionnaire to students attending a local public high school
-   (in the U.S. high school is grades 9-12, which is mostly students from ages 14 to 18.)
-   Explain how this sample is biased.
-   Who has a decreased or even zero probability of being selected.

*Type your ideas in the chat box.*

::: notes
*Speaker notes*

Here is a scenario where a researcher selects a biased sample. I should note here that this is an example specific to the United States. In Italy, you might talk about a survey distributed to the scuola secondaria di secondo grado.

STOP AND GET STUDENT RESPONSES

There are a variety of responses here. The sample does not include home schooled students, students in private schools, students with chronic diseases that force frequent school absences, and students who have dropped out.
:::

## Fixing a biased sample

-   Redfine your population
    -   Not all teenagers,
        -   but those attending public high schools.

## What is a parameter?

-   A parameter is a number computed from a population.
    -   Examples
        -   Average health care cost associated with the 29,637 children
        -   Proportion of these 29,637 children who died in their first year of life.
        -   Correlation between gestational age and number of ER visits of these 29,637 children.
    -   Designated by Greek letters ($\mu$, $\pi$, $\rho$)

## What is a statistic?

-   A statistic is a number computed from a sample
    -   Examples
        -   Average health care cost associated with 100 children.
        -   Proportion of these 100 children who died in their first year of life.
        -   Correlation between gestational age and number of ER visits of these 100 children.
    -   Designated by non-Greek letters ($\bar{X}$, $\hat{p}$, r).

## What is Statistics?

-   Statistics
    -   The use of information from a sample (a statistic) to make inferences about a population (a parameter)
        -   Often a comparison of two populations

## What is the null hypothesis?

-   The null hypothesis ($H_0$) is a statement about a parameter.
-   It implies no difference, no change, or no relationship.
    -   Examples
        -   $H_0:\ \mu_1 - \mu_2 = 0$
        -   $H_0:\ \pi_1 - \pi_2 = 0$
        -   $H_0:\ \rho = 0$

## What is the alternative hypothesis?

-   The alternative hypothesis ($H_1$ or $H_a$) implies a difference, change, or relationship.
    -   Examples
        -   $H_1:\ \mu_1 - \mu_2 \ne 0$
        -   $H_1:\ \pi_1 - \pi_2 \ne 0$
        -   $H_1:\ \rho \ne 0$

## Hypothesis in English instead of Greek

-   Only statisticians like Greek letters
    -   Translate to simple text
    -   For two group comparisons
        -   Safer, more effective
    -   For regression models
        -   Trend, association

::: notes
*Speaker notes*

As a researcher, you should always think about your hypothesis in terms of population parameters, but your writing should use text. Translate the Greek letters to English.

If you have a hypothesis that compares two groups, look for comparative words like "safer" or "more effective". If your hypothesis involves some type of regression model, you should consider terms like "trend" or "association".
:::

## Use PICO

-   P = patient population
-   I = intervention
-   C = control
-   O = outcome

## Example of text hypotheses (1/2)

-   "... the objective of this 78-week randomised, placebo-controlled study was to determine whether treatment with nilvadipine sustained-release 8 mg, once a day, was effective and safe in slowing the rate of cognitive decline in patients with mild to moderate Alzheimer disease."
    -   Lawlor B, Segurado R, Kennelly S, et al. Nilvadipine in mild to moderate Alzheimer disease: A randomised controlled trial. PLoS Med. 2018; 15(9): e1002660. DOI: 10.1371/journal.pmed.1002660

::: notes
*Speaker notes*

Here's an example of a two group comparison. One group gets nilvadipine and the other group gets a placebo. Safety was measured as the proportion of patients who experienced an adverse event. The researchers also measured the proportion of patients who experienced a serious adverse event. So the Greek hypothesis would involve pi's.

Effectiveness was measured using the Alzheimer's Disease Assessment Scale Cognitive Subscale-12 and the Clinical Dementia Rating Scale sum of boxes. Both of these outcome measurements are continuous, so the Greek hypothesis would involve mu's.
:::

## PICO for this study

-   P = patients with mild to moderate Alzheimer disease
-   I = Nilvadine
-   C = placebo
-   O = cognitive function

## Example of text hypotheses (2/2)

-   "... we investigated trends in BCC incidence over a span of 20 years and the associations between incident BCC and risk factors in a total population of 140,171 participants from 2 large US-based cohort studies: women in the Nurses' Health Study (NHS; 1986--2006) and men in the Health Professionals' Follow-up Study (HPFS; 1988--2006)."
    -   Wu S, Han J, Li WQ, Li T, Qureshi AA. Basal-cell carcinoma incidence and associated risk factors in U.S. women and men. Am J Epidemiol. 2013; 178(6): 890--897. DOI: 10.1093/aje/kwt073

::: notes
*Speaker notes*

This study used a regression model, a Cox regression model, to study trends and associations, so the Greek hypotheses would involve beta's.
:::

## PICO for this study

-   P = female nurses/male health professionals
-   I = various risk factors
-   C = absence of various risk factors
-   O = presence/absence of BCC

## One-sided alternatives

-   Examples
    -   $H_1:\ \mu_1 - \mu_2 \gt 0$
    -   $H_1:\ \pi_1 - \pi_2 \gt 0$
    -   $H_1:\ \rho \gt 0$
-   Changes in only one direction expected
-   Changes in opposite direction uninteresting

## Passive smoking controversy

-   EPA meta-analysis of passive smoking
    -   Criticized for using a one-sided hypothesis
    -   Samet JM, Burke TA. Turning science into junk: the tobacco industry and passive smoking. Am J Public Health. 2001;91(11):1742--1744.

::: notes
*Speaker notes*

Available in [html format](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1446866/) or [PDF format](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1446866/pdf/0911742.pdf).

Consider a study of the effects of second-hand smoke. These studies always use directional alternatives. From what we know about active cigarette smoking is that it increases the risk of cancer and cardiovascular disease. So there is no reason to expect that passive smoke exposure should be any different than active smoking. Maybe it is less toxic, because of dilution and because the smoking coming off a cigarette from one end is different than the smoke coming off the cigarette from the other end. Fair enough, but there is not reason to believe that things are so different that all of a sudden the smoke becomes protective.

Since there is no scientific basis for a protective effect of passive smoking, it makes sense to test that passive smoking has no effect versus it having an increase in bad outcomes compared to the control group. So your null hypothesis is "not harmful" and your alternative is "harmful". The beneficial hypothesis is lumped into the null hypothesis, but no one would dare claim that passive smoking was protective.

Actually, the tobacco companies did complain that the use of a directional alternative violated the norms of science. They won in a court battle in North Carolina, but lost on appeal.

As another aside, I was involved with prayer study. We planned this study using a one-sided hypothesis (remote prayer has a positive effect on health). The Institutional Review Board suggested changing this to a two-sided hypothesis (remote prayer has either a positive or a negative effect on health). Thankfully, we did not observe an outcome in the opposite tail as that would have been very difficult to explain.
:::

## What is a decision rule? (1/3)

-   Example
    -   $H_0:\ \mu_1 - \mu_2 = 0$
    -   $H_1:\ \mu_1 - \mu_2 \ne 0$
    -   t = ($\bar{X}_1-\bar{X}_2$) / se
    -   Accept $H_0$ if t is close to zero.

## What is a decision rule? (2/3)

-   Example
    -   $H_0:\ \pi_1 - \pi_2 = 0$
    -   $H_1:\ \pi_1 - \pi_2 \ne 0$
    -   t = ($\hat{p}_1-\hat{p}_2$) / se
    -   Accept $H_0$ if t is close to zero.

## What is a decision rule? (3/3)

-   Example
    -   $H_0:\ \rho = 0$
    -   $H_1:\ \rho \ne 0$
    -   t = r / se
    -   Accept $H_0$ if t is close to zero.

## What is a Type I error?

-   A Type I error is rejecting the null hypothesis when the null hypothesis is true
    -   False positive
    -   Example involving drug approval: a Type I error is allowing an ineffective drug onto the market.
-   $\alpha$ = P\[Type I error\]

::: notes
*Speaker notes*

In your research, you specify a null hypothesis (typically labeled H0) and an alternative hypothesis (typically labeled Ha, or sometimes H1). By tradition, the null hypothesis corresponds to no change. When you are using Statistics to decide between these two hypothesis, you have to allow for the possibility of error. Actually, if you are using any other procedure, you should still allow for the possibility of error, but we statisticians are the only ones honest enough to admit this.

A Type I error is rejecting the null hypothesis when the null hypothesis is true.

Consider a new drug that we will put on the market if we can show that it is better than a placebo. In this context, H0 would represent the hypothesis that the average improvement (or perhaps the probability of improvement) among all patients taking the new drug is equal to the average improvement (probability of improvement) among all patients taking the placebo. A Type I error would be allowing an ineffective drug onto the market.

Remember that the hypotheses involve population parameters. Population parameters are impossible to compute. So you can only talk about Type I errors in an abstract sense. You will never know for certain if you have made a Type I error.

Alpha is the probability of a Type I error, and alpha is a value that you can compute. In most studies, researchers work hard to keep the probability of a Type I error low, typically at 5%.
:::

## What is a Type II error?

-   A Type II error is accepting the null hypothesis when the null hypothesis is false.
    -   False negative result
    -   Usually computed at MCD
    -   An example involving drug approval: a Type II error is keeping an effective drug off of the market.
-   $\beta$ = P\[Type II error\]
-   Power = $1-\beta$

::: notes
*Speaker notes*

A Type II error is accepting the null hypothesis when the null hypothesis is false. You should always remember that it is impossible to prove a negative. Some statisticians will emphasize this fact by using the phrase "fail to reject the null hypothesis" in place of "accept the null hypothesis." The former phrase always strikes me as semantic overkill.

Many studies have small sample sizes that make it difficult to reject the null hypothesis, even when there is a big change in the data. In these situations, a Type II error might be a possible explanation for the negative study results.

Consider a new drug that we will put on the market if we can show that it is better than a placebo. In this context, H0 would represent the hypothesis that the average improvement (or perhaps the probability of improvement) among all patients taking the new drug is equal to the average improvement (probability of improvement) among all patients taking the placebo. A Type II error would be keeping an effective drug off the market.

It bears repeating that population parameters are impossible to compute. So you will never know for certain if you have made a Type I error.

Beta is the probability of a Type II error. Beta is a known quantity. Typically researchers try to keep beta small. 10% is a typical value, though in some settings, a Type II error rate as large as 20% could be tolerated.

Power is defined as 1-beta. I will talk more about power in a little bit.
:::

## What is a p-value?

-   Let t =
    -   ($\bar{X}_1-\bar{X}_2$) / se, or
    -   ($\hat{p}_1-\hat{p}_2$) / se, or
    -   r / se
-   p-value = Prob of sample result, t, or a result more extreme,
    -   **assuming the null hypothesis is true**
-   Small p-value, reject $H_0$
-   Large p-value, accept $H_0$

::: notes
*Speaker notes*

A p-value is a measure of how much evidence we have against the null hypothesis.

The smaller the p-value, the more evidence we have against H0.

The p-value is also a measure of how likely we are to get a certain sample result or a result "more extreme," assuming H0 is true.

The type of hypothesis (right tailed, left tailed or two tailed) will determine what "more extreme" means.
:::

## Alternate interpretations

-   Consistency between the data and the null
    -   Small value, inconsistent
    -   Large value, consistent
-   Evidence against the null
    -   Small, lots of evidence against the null
    -   Large, little evidence against the null

::: notes
*Speaker notes*

There are two interpretations that I feel are more practical. You can think of the p-value as a measure of consistency between the data and the null hypothesis. A small value implies inconsistency. It is very unlikely that you will get a value like you've seen in your sample or a value more extreme under the assumption that the null hypothesis is true. So you should reject that assumption.

On the other hand if the sample results or anything more extreme has a high probability under the assumption that the null hypothesis is true, then you should feel comfortable accepting that assumption.

I have argued that the p-value is a measure of evidence. Some have called it a poor measure of evidence, but I stand by my interpretation.

If the p-value is small, you have lots of evidence against the null hypothesis. If the p-value is large, you have little or no evidence against the null hypothesis.
:::

## What the p-value is not (1/2)

-   A p-value is NOT the probability that the null hypothesis is true.
    -   P\[t or more extreme \| null\] is different than
    -   P\[null \| t or more extreme\]
        -   P\[null\] is nonsensical
        -   $\mu$, $\pi$, or $\rho$ are unknown constants (no sampling error)

::: notes
*Speaker notes*

The p-value is a conditional probability, and you always need to be careful about conditional probabilities. It is a probability about a sample result given an assumption about the population result. It is not a probability about a population result given the sample result. There are two reasons for this.

First, you can't reorder a conditional probability. The probability of A given B is almost never the same as the probability of B given A. The example I give for this is the probability of being happy given that you are rich. That's a pretty high number, I hope you'll agree. There are a few rich people who lead miserable lives, but from everything I've seen, most rich people are pretty darn happy. The reverse of this is the probability of being rich given that you are happy. That number is much smaller. Because although I believe that money can buy happiness, a lot of other things can also buy happiness just as well. It's not quite as easy to find happiness if you're poor, but somehow, a lot of poor people find a way to be happy anyway.

A second reason that you can't reverse the order is that you cannot make a probability statement about population parameters. They are numbers computed from the entire population, and are fixed values. You cannot make a probability statement about something that has no sampling error.

Only numbers computed from a sample (i.e., statistics) have sampling error.
:::

## What the p-value is not (2/2)

-   Not a measure FOR either hypothesis
    -   Little evidence **against** the null $\ne$ lots of evidence **for** the null
-   Not very informative if it is large
    -   Need a power calculation, or
    -   Narrow confidence interval
-   Not very helpful for huge data sets

::: notes
*Speaker notes*

The p-value is not a measure for either hypothesis. It is always a measure against a particular hypothesis. Now when the p-value is small, you can make a strong statement. We have lots of evidence against the null hypothesis. That translates into lots of evidence in favor of the alternative hypothesis.

When the p-value is large, however, you are in a quandary. Little or no evidence against the null hypothesis is not the same as lots of evidence for the null hypothesis.

It's possible to have little or no evidence against the null and also have little or no evidence against the alternative. This happens whenever you have a really small sample size combined with a lot of noise.

You can't prove a negative, so the saying goes. Well, you can prove a negative, but you have to work harder at it. A large p-value by itself is not persuasive, but if you combine it with a power calculation done prior to data collection, that's pretty good evidence in support of the null hypothesis.

You could also combine a large p-value with a narrow confidence interval to support the null hypothesis. I'll talk about that more in just a bit.

In general, the p-value is not very helpful for large samples. We're seeing this more and more. Just about everything pops up as statistically significant with these huge data sets, and you can't use the p-value to separate the important stuff from the trivial stuff. You need to look instead at the magnitude of the sample estimates and calculate how much uncertainty you can remove in your future predictions.
:::




A research paper computes a p-value of 0.45. How would you interpret this p-value?

1.  Strong evidence **for** the null
2.  Strong evidence **for** the alternative
3.  Little or no evidence **for** the null
4.  Little or no evidence **for** the alternative
5.  More than one answer above is correct.
6.  I do not know the answer.

::: notes
*Speaker notes*

Here's that pop quiz again. Take a look at it quickly. Note that the p-value is of evidence against the null hypothesis. So each of the first four responses is wrong.

I wrote this question quickly, so shame, shame on me. But I've reproduced the example because it illustrates an important point.
:::

## 

![xkcd cartoon about jelly beans and cancer](../images/xkcd-jelly-beans.png){#fig-xkcd-jelly-beans fig-align="left"}

::: notes
*Speaker notes*

This cartoon is impossible to read, but you can find it on the Canvas site or in the readings. Here's a brief run down.

In the first panel, a woman runs up to a man and shouts: Jelly beans cause acne!

The man replies : Scientists! Investigate!

In the second panel, one scientist, holding a clipboard announces: We found no link between jelly beans and acne (p \> 0.05).

In the third panel, the woman says: I hear it's only a certain color that causes it.

In a bunch of small panels, the scientist with a clipboard reports: We found no link between purple jelly beans and acne (p \> 0.05).

We found no link between brown jelly beans and acne (p \> 0.05).

We found no link between pink jelly beans and acne (p \> 0.05).

The same for blue, teal, salmon, red, and so forth. And then...

We found a link between green jelly beans and acne (p \< 0.05). An off-screen voice goes: Whoa!

The next six panels show

We found no link between mauve jelly beans and acne (p \> 0.05).

We found no link between beige jelly beans and acne (p \> 0.05).

We found no link between lilac jelly beans and acne (p \> 0.05).

We found no link between black jelly beans and acne (p \> 0.05).

We found no link between peach jelly beans and acne (p \> 0.05).

We found no link between orange jelly beans and acne (p \> 0.05).

At the bottom is a newspaper with the headline: Green Jelly Beans Linked To Acne! 95% Confidence. Only 5% chance of coincidence!

If you are interested in a transcript and a detailed explanation, https://www.explainxkcd.com/wiki/index.php/882:\_Significant
:::

## What is p-hacking?

-   Abuse of the hypothesis testing framework.
    -   Run multiple tests on the same outcome
    -   Test multiple outcome measures
    -   Remove outliers and retest
-   Defenses against p-hacking
    -   Bonferroni
    -   Primary versus secondary
    -   Published protocol

::: notes
*Speaker notes*

This is an example of p-hacking. You change the testing process to increase the probability of a Type I error (Rejecting the null hypothesis when the null hypothesis is true). This increases the chance of getting a positive result, which you may find desirable, but only by increasing the probability of a false positive result.

Some examples of p-hacking. Run multiple tests on the same outcome measure. Start with the regular t-test, include the t-test that allows for unequal variances, and run two different non-parametric tests, the Wilcoxon-Mann-Whitney test and the sign test. Choose the test with the smallest p-value.

You also might consider multiple outcome measures. Compare the mortality rate, the relapse rate, and the re-hospitalization rate. If any of the three is statistically significant, claim victory.

You could also do this with longitudinal data. Compare pain relief at one hour and at four hours. If you see a difference at one hour, claim that your new medication is faster acting. If you see a difference at four hours, claim that your medication is longer lasting.

You might run a test with the full data set and then with an outlier or two removed. Report for the data set that has the smaller p-value and pretend that this was your original choice all along.

These are only a few of the choices. I don't want to say more because I feel like I'm the devil tempting you.

There are two defenses against p-hacking. Well three if you count being honest. But what I mean is there are two things that you can do that will satisfy others that you are playing fairly.

First, you can adjust your decision rule by using a Bonferroni correction. Bonferroni divides alpha by the number of tests. If you are using three different outcome measures, compare your p-value of 0.0133 instead of 0.05.

Second, you can designate one of your outcome measures as primary. If you achieve statistical significance on your primary outcome, great. The remaining outcome measures are secondary. If you achieve statistical significance on a secondary outcome meausure only, report the results as provisional and requiring independent replication.

You should publish a detailed protocol, either through a clinical trial registry, or now there are journals which accept publications of the research protocols before any data are collected. It's a paper with literature review and methods section, but no results and no discussion section.

Now p-hacking has happened because some people have a skewed view of research. They are interested in using research to promote their own agenda rather than using research to uncover the truth. Perfectly understandable if you are a drug company, but you as an independent researcher should never try to skew the data. It hurts you and it hurts your patients. You need to adopt a disinterested posture in that you are glad when the research points in one direction and you are glad when it points in the opposite direction, because either way, you know more than you did before and you can treat your patients better because of this knowledge.
:::
