---
title: "Comments for MEDB 5501, Week 6"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: visual
---

## Assessing normality

+ Problems caused by non-normality
  + Poor confidence intervals, hypothesis tests
    + Too much imprecision
    + Poor coverage probability
      + Especially for one tailed tests
  + Inability to extrapolate
+ What about the Central Limit Theorem?

::: notes
Sometimes, I think that researchers obsess too much about non-normality, but in fairness to them, it is an important issue. If your data does not follow a bell shaped curve, then several problems could happen.

First, you might see a greater degree of imprecision, reflected in very wide confidence intervals and loss of statistical power.

Second, you might have poor coverage probability. The 95% confidence interval might only reprent a 92% confidence level. The Type I error rate might be greater than 5%.

Third, and this point is not emphasized enough, is that non-normal data makes it difficult for you to extrapolate to future observations. Prediction of future events is a big part of Statistics. It is difficult even with normal data and becomes much more difficult with non-normal data.

Yes, you might say, but doesn't the Central Limit Theorem help us out? Well, yes, if the sample size is large, but it only helps with assuring accurate confidence levels and good control of the Type I error rate. Non-normal data will still often produce intervals that are too wide and tests that have too little power.
:::

## How to handle non-normality

+ Ignore it
  + Central Limit Theorem
+ Transform your data
+ Use alternatives
  + Nonparametric tests (covered in a later module)
  + Bootstrap (covered in a later module)
  + Randomization tests (not covered in this class)

::: notes
Yes, you might say, but doesn't the Central Limit Theorem help us out? Well, yes, but it only helps with assuring accurate confidence levels and good control of the Type I error rate. Non-normal data will still often produce intervals that are too wide and tests that have too little power.

Still, there often is benefit in not worrying so much about non-normality. If your sample size is large and the deviations from normality are minor, don't let this keep you up at night.

Even so, there are some benefits to addressing non-normality. Transformations, which I will cover in just a bit, can often help out tremendously. There are also alternatives to the simple tests using a sample mean: nonparametric tests and the bootstrap. I will cover those in a later module. There are also randomization tests, which I don't think are covered. I've given a talk on randomization tests and I have a few webpages that talk about this topic.
:::

```{r}
suppressMessages(suppressWarnings(library(ggplot2)))
hn <- function(n, m) {
  ymax <- n*(pnorm(m)-pnorm(-m))
  x <- rnorm(n)
  ggplot(data.frame(x), aes(x)) +
    geom_histogram(
        binwidth=m,
        center=0,
        fill="white",
        color="black") + 
    expand_limits(x=c(-6, 6)) +
    expand_limits(y=ymax) +
    scale_x_continuous(breaks=-6:6) +
    scale_y_continuous(name=NULL, labels=NULL)
}
n <- 100
m <- 1
```

## Normal histogram with n=`r format(n, scientific=FALSE, big.mark=",")`

```{r}
hn(n, m)
n <- 10*n
m <- m/2
```

::: notes
I'm going to show a sequence of histograms where the sample size increases and the width of the bars simultaneously decreases.
:::

## Normal histogram with n=`r format(n, scientific=FALSE, big.mark=",")`

```{r}
hn(n, m)
n <- 10*n
m <- m/2
```

## Normal histogram with n=`r format(n, scientific=FALSE, big.mark=",")`

```{r}
hn(n, m)
n <- 10*n
m <- m/2
```

## Normal histogram with n=`r format(n, scientific=FALSE, big.mark=",")`

```{r}
hn(n, m)
n <- 10*n
m <- m/2
```

## Normal histogram with n=`r format(n, scientific=FALSE, big.mark=",")`

```{r}
hn(n, m)
n <- 10*n
m <- m/2
```

```{r}
dn <- function(mu, sigma, k=2000) {
  # A gamma with a large shape parameter is almost
  # identical to a normal distribution.
  x <- seq(-6, 6, length=1000)
  y <- dnorm(x, mu, sigma)
  y <- dgamma(x-mu+sqrt(k)*sigma, k, sqrt(k)/sigma)
  z <- dnorm(x, 0, 1)
  ymax <- dnorm(0, 0, 0.5)
  ggplot(data.frame(x, y, z), aes(x, z)) +
    geom_line(color="gray", linewidth=1.5) +
    geom_line(aes(x, y), color="black", linewidth=1.5) +
    expand_limits(y=ymax) +
    scale_x_continuous(breaks=-6:6) +
    scale_y_continuous(name=NULL, labels=NULL)
}
mu <- 0
sigma <- 1
```

## Normal distribution (n=infinity)

```{r}
dn(mu, sigma)
mu <- 2
```

::: notes
It's impossible to show an infinite number of bars with zero width, but if you could, it would look like this.

This is the curve that you see in your text box and many other sources. It is a theoretical curve becuase

(a) it represents a mathematical limit, and 

(b) no reall world data will ever match the smoothness and symmetry that you see here. Some data sets do come close, though.

This curve is for the standard normal distribution, a distribution with mu=0 and sigma=1. There are other distributions, which I will show in the upcoming slides.
:::

## Normal(`r mu`, `r sigma`)

```{r}
dn(mu, sigma)
mu <- -1
```



## Normal(`r mu`, `r sigma`)

```{r}
dn(mu, sigma)
mu <- 0
sigma=2
```

## Normal(`r mu`, `r sigma`)

```{r}
dn(mu, sigma)
mu <- 0
sigma=0.5
```

## Normal(`r mu`, `r sigma`)

```{r}
dn(mu, sigma)
```

::: notes
You can stretch of squeeze the normal curve, you can shift it to the left or to the right, but it still maintains a characteristic shape.
:::

## Skewed right

```{r}
mu <- 0
sigma <- 1
dn(mu, sigma, k=5)
```

::: notes
Here's an example of a distribution that is skewed to the right. I have scaled this distribution so that the mean is zero and the standard deviation is one.

There are three key features for a skewed right distribution.

First, the chances of seeing an outlier on the low end (more than two standard deviations below the mean) is less for a skewed right distribution. Skewed right distributions almost never produce outliers on the low end.

Second, the chances of seeing an outlier on the high end (more than two standard deviations above the mean) is greater for a skewed right distribution.

Third, the probability of any value being less than the mean is a bit more than 50%. That's because the outliers on the high end of this distribution tend to have an undue influence on the mean, making it bigger than the bulk of the data.
:::

## Right skewness is characterized by the tails of the distribution

+ Heavy right tail
  + Greater tendency to produce extreme values on the right
+ Light left tail
  + Lesser tendency to produce extreme values on the left
+ Right skewness is the most common type of non-normality

::: notes
A right skewed distribution means a heavy tail on the right and a light tail on the left. A heavy right tail means lots of probability associated with extreme values on the right (the high end). A light left tail means much less probability associated with extreme values on the left (the low end).

Recognize this pattern right away because it is the most common deviation from normality.
:::

## Normal probability plot

+ Compare  data to evenly spaced percentiles of the normal distribution
+ Example with n=4
  + Compare smallest value with $Z_{0.2}$
  + Compare next value with $Z_{0.4}$
  + Compare next value with $Z_{0.6}$
  + Compare largest value with $Z_{0.8}$
+ No best definition for evenly spaced
  + 12.5, 37.5, 62.5, 87.5, for example
  
::: notes
The normal probability plot is an excellent way to examine whether data is normally distributed. Compare each value to the corresponding percentile of a normal distribution.

For a simple example with four data points, compare your values to the 20th, 40th, 60th, and 80th percentiles.

There are alternative ways to define evenly spaced. They might lead to slightly different plots, but they are not worth fussing over. Any definition of evenly spaced works just fine.
:::

## Right skewed data

:::: {.columns}

::: {.column width="50%"}
![](../images/histograms-narrow-0.png)
![](../images/qq-plots-0.png)
:::

::: {.column width="50%"}
![](../images/histograms-wide-0.png)

![](../images/skew-kurt-0.png)
:::

::::

::: notes
Here is some artificial data the has a right skew.
:::

## Left skewed data

:::: {.columns}

::: {.column width="50%"}
![](../images/histograms-narrow-1.png)
![](../images/qq-plots-2.png)
:::

::: {.column width="50%"}
![](../images/histograms-wide-1.png)

![](../images/skew-kurt-1.png)
:::

::::

::: notes
It is less common, but sometimes you may see left skewed data. This is heavy tailed on the left and light tail on the right. This means that if you see outliers, they tend to appear on the left (the low end).
:::

## Heavy tailed data

:::: {.columns}

::: {.column width="50%"}
![](../images/histograms-narrow-2.png)
![](../images/qq-plots-4.png)
:::

::: {.column width="50%"}
![](../images/histograms-wide-2.png)

![](../images/skew-kurt-2.png)
:::

::::

::: notes
Sometimes you see an excess of extreme values on both ends. This is a heavy tail on the left and a heavy tail on the right.
:::

## Light tailed data

:::: {.columns}

::: {.column width="50%"}
![](../images/histograms-narrow-3.png)
![](../images/qq-plots-6.png)
:::

::: {.column width="50%"}
![](../images/histograms-wide-3.png)

![](../images/skew-kurt-3.png)
:::

::::

::: notes
You might see a light tailed distribution. This means there are fewer extremes at either end. The data just drops off suddenly in both directions.
:::

## Bimodal data

:::: {.columns}

::: {.column width="50%"}
![](../images/histograms-narrow-4.png)
![](../images/qq-plots-8.png)
:::

::: {.column width="50%"}
![](../images/histograms-wide-4.png)

![](../images/skew-kurt-4.png)
:::

::::

::: notes
Bimodal data is tricky to diagnose with a histogram because what looks bimodal with a large number of bars can look quite different with fewer bars.
:::

## Normal data

:::: {.columns}

::: {.column width="50%"}
![](../images/histograms-narrow-5.png)
![](../images/qq-plots-10.png)
:::

::: {.column width="50%"}
![](../images/histograms-wide-5.png)

![](../images/skew-kurt-5.png)
:::

::::

::: notes
Not everything is non-normal. Here is what you would see with the various diagnostics for normal data.
:::
