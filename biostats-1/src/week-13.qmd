---
title: "Comments for MEDB 5501, Week 13"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: visual
---

## Covariance

-   $Cov(X,Y)=\frac{1}{n-1}\Sigma(X_i-\bar{X})(Y_i-\bar{Y})$
    -   $(X_i-\bar{X})(Y_i-\bar{Y})$ is positive if
        -   $X_i$ and $Y_i$ both above average
        -   $X_i$ and $Y_i$ both below average
    -   $(X_i-\bar{X})(Y_i-\bar{Y})$ is negative if
        -   $X_i$ above average and $Y_i$ below average
        -   $X_i$ below average and $Y_i$ above average

::: notes
I want to start this section with a discussion of covariance. Covariance is a term that the more mathematically oriented statisticians love to use. It is an interesting statistic from a theoretical perspective and it forms the foundation for a large number of statistical tests.

It doesn't, however, have as much practical application, compared to the correlation, which you will see in just a bit. I am introducing it here to get you familiar with the terminology.

It is sort of analogous to the term variance versus standard deviation. The variance is interesting from a theoretical perspective, but the standard deviation has far more practical implications.

To compute the covariance, you add up a bunch of terms, each of which is a product. The product is positive if both X and Y are above average or if both X and Y are below average. A positive times a positive is positive and a negative times a negative is also positive.

The product is negative if one value is above average and the other value is below average. A positive times a negative or a negative times a positive produces a negative product.

Think of the covariance as measuring the tendency for two variables to co-vary in a positive sense (large pairing with large and small pairing with small) or in a negative sense (large pairing with small and small pairing with large).
:::

## 

```{r}
library(tidyverse)
y <- 6 + 4*(1:6)
e <- c(4, -2, -6, 6, -6, 4)
x <- rev(y) + 2*e + 20
x <- x/2
y <- y/2
o <- order(x)
data.frame(x, y) %>%
  arrange(x) %>%
  print(row.names=FALSE)
xbar <- mean(x)
ybar <- mean(y)
sx <- round(sd(x), 1)
sy <- round(sd(y), 1)

fn <- "../data/data-13-correlation.csv"
write_csv(
  data.frame(x, y), 
  file=fn)
```

$\ $

 $\bar{X}=`r xbar`$;
 
 $\bar{Y}=`r ybar`$;
 
 $S_X=`r sx`$;
 
 $S_Y=`r sy`$

##

::: notes
Here is an artificial data set that I chose to make some of the calculations simpler. There are two variables, X and Y, and you want to measure how much they co-vary.
:::

```{r}
data.frame(x, y) %>%
  ggplot(aes(x, y)) + 
    scale_y_continuous(
      expand=expansion(add=0.75),
      breaks=2*(0:10)) +
    scale_x_continuous(
      expand=expansion(add=2),
      breaks=10+2*(0:10)) -> base_plot
base_plot +
    geom_point(size=3)
```

::: notes
Here is a plot of the data. Notice a slight downward trend in the data. Admittedly, there isn't much data, but bear with me on this.
:::

## 

```{r}
zx <- paste0(ifelse(x-xbar>0, "+", ""), x-xbar)
zy <- paste0(ifelse(y-ybar>0, "+", ""), y-ybar)
lab <- paste0("(",zx, ",", zy, ")")
base_plot +
    geom_segment(
      x=mean(x), 
      y=min(y)-0.5,
      xend=mean(x), 
      yend=max(y)+0.5,
      linetype=2) +
    geom_segment(
      x=min(x)-1.5, 
      y=mean(y),
      xend=max(x)+1.5, 
      yend=mean(y),
      linetype=2) +
    geom_text(label=lab)
```

::: notes
The covariance measures how much each value deviates from the mean. Notice for two of the data points in the upper left corner of the graph, the X value is below average and the Y value is above average. 

There is only one data point in the lower left corner, representing a data value where both X and Y are below average.

There are two points in the lower right corner, representing a data value where X is above average and Y is below average.

Finally, there is a single point in the upper right corner, representing a data value where both X and Y are above average.
:::

## Calculation of covariance

```{r}
d <- data.frame(
  x_centered=x-xbar, 
  y_centered=y-ybar, 
  product=(x-xbar)*(y-ybar))
print(d, row.names=FALSE)
total <- sum(d$product)
```

$\ $

-   $Cov(X,Y)=\frac{1}{5}(`r total`)=`r var(x,y)`$

::: notes
Take the products of the terms in the previous graph, add them up and divide by n-1. In this example, n-1=5.
:::

## Correlation

-   $Corr(X,Y)=\frac{Cov(X,Y)}{S_XS_Y}$
    -   Also use $r_{XY}$
    -   Population correlation is $\rho_{XY}$

::: notes
The correlation is just the covariance divided by the two standard deviations. This calculation makes the quantity unitless, which is both an advantage and a disadvantage (but mostly an advantage).
:::

## Calculation of correlation

-   $r_{XY}=\frac{`r var(x,y)`}{`r sx` \times `r sy`}=`r cor(x,y)`$
    -   Always round!
        -   $r_{XY}=`r round(cor(x,y), 2)`$ or $`r round(cor(x,y), 1)`$
        
::: notes
Here is an example of how to compute a correlation. It's easy once you have the covariance. Be sure to round your correlations to two decimal places. I'm in a minority here, but I often think that rounding to a single decimal place is appropriate. It may be a bit extreme, but you'll see some examples where this amount of rounding makes it much easier to see patterns.
:::

## Interpretation of correlation

-   r is always between -1 and +1
    -   Positive values imply positive association
    -   Negative values imply negative association
    -   Strongest associations closest to -1 or +1
    
::: notes
The correlation is always between -1 and +1. That's because two measurements cannot co-vary more than the variation that the individual measurements have. It's pretty easy to show this, actually, but I won't do it here.
:::

## r between -1 and -0.7, strong negative association

```{r}
gen <- function(r, n=100) {
  x <- rnorm(n)
  x <- (x - mean(x))/sd(x)
  y <- rnorm(n)
  e <- resid(lm(y~x))
  e <- (e - mean(e))/sd(e)
  y <- sign(r)*(x+e*sqrt((1/r^2-1)))
  y <- (y- mean(y))/sd(y)
  data.frame(x, y) %>%
    ggplot(aes(x, y)) + 
      geom_point(shape=1, size=3) +
      expand_limits(x=c(-3,3), y=c(-3,3)) +
    ggtitle(
      paste0(
        "Example of data with r =",
        ifelse(r > 0, " +"," "),
        round(cor(x,y),1)))
}
```

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(-0.9)
```
:::
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(-0.8)
```
:::
::::

::: notes
A correlation close to -1 indicates a strong negative relationship or association. Here are two artificial examples illustrating what a correlation of -0.9 and -0.8 look like.
:::
  
## r between -0.7 and -0.3, weak negative association   

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(-0.6)
```
:::
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(-0.4)
```
:::
::::

::: notes
A correlation between -0.3 and -0.7 indicates a weak negative relationship or association. Here are two examples with correlations of -0.6 and -0.4.
:::

## r between -0.3 and +0.3, little or no association

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(-0.2)
```
:::
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(0.2)
```
:::
::::

::: notes
A correlation between -0.3 and 0.3 indicates little or no association. Here are two examples with correlations of -0.2 and +0.2.
:::

## r between +0.3 and +0.7, weak positive association   

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(0.4)
```
:::
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(0.6)
```
:::
::::

::: notes
A correlation between +0.3 and +0.7 indicates a weak positive association. Here are two examples with correlations of +0.4 and +0.6.
:::

## r between +0.7 and +1, strong positive association   

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(0.8)
```
:::
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(0.9)
```
:::
::::

::: notes
A correlation close to +1 indicates a strong positive association. Here are two examples with correlations of 0.8 and 0.9.
:::

## Extreme case, perfect association

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(1)
```
:::
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 4
gen(-1)
```
:::
::::

::: notes
A correlation equaling +1 or -1 exactly implies a perfect association.
:::

## Sleep data dictionary, 1 of 6

```{r}
fn <- "../data/data-13-sleep-data-dictionary.yaml"
ft <- readLines(fn)
i <- 1:14
cat(ft[i], sep="\n")
```

::: notes
You will see some practical applications of correlation using a dataset from OzDASL looking at sleep patterns in mammals.
:::

## Sleep data dictionary, 2 of 6

```{r}
i <- 15:30
cat(ft[i], sep="\n")
```

::: notes
Note that there is no information about how you might use or share this data. This is a common problem with data sources on the Internet.
:::

## Sleep data dictionary, 3 of 6

```{r}
i <- 31:37
cat(ft[i], sep="\n")
```

::: notes
Here's the interesting thing about this data. It has missing value codes. I want to talk about missing values in more detail in just a bit.
:::

## Sleep data dictionary, 4 of 6

```{r}
i <- 38:49
cat(ft[i], sep="\n")
```

::: notes
Here's the information on the first few variables...
:::

## Sleep data dictionary, 5 of 6

```{r}
i <- 50:64
cat(ft[i], sep="\n")
```

::: notes
...and the next set of variables...
:::

## Sleep data dictionary, 6 of 6

```{r}
i <- 65:79
cat(ft[i], sep="\n")
```

::: notes
...and the last few variables.
:::

## What does a missing value represent

-   Dropout
-   Refuse to answer survey question
-   Survey question is not applicable
-   Lab result is lost
-   Concentration below detectable limit
-   Many other reasons

::: notes
There are many reasons why a data value might be designated as missing. If you are involved with data analysis, you need to understand WHY a data value is missing and adjust the statistical analysis plan appropriately. How you adjust your plan is difficult to say. It does depend a lot on the context.
:::

## Common missing value codes

-   A single dot (.)
    -   SPSS and SAS
-   NA
    -   R
-   Asterisk (*) and other symbols
-   Unusual number codes (-1, 9, 99, 999)

::: notes
There are a variety of codes for missing values. You will see all of these if you work with data long enough.

A single dot is common, and is the default option in SPSS and SAS. The letters "NA" are also common. This is the default for the R programming language. I've seen an asterisk used frequently for missing values.

Also common is the use of unusual number codes. These are numbers outside the range of reasonable values. A negative value is common for many variables that can only take on positive values. A birthweight of -1, for example, either means missing or a baby that floats to the ceiling after it is born.

For other variables a field with one, two, or three nines is common.
:::

## Importing missing values

-   No problems for default value
-   NA and * convert numeric to string
    -   Fix during import, or
    -   Convert back after import
-   Unusual number codes
    -   Designate after import
    -   Don't forget!

::: notes
When you are importing a dataset with missing data into SPSS, use of the default code, a single dot, will usually work just fine.

The problem occurs when the data you get uses a different code, like NA or asterisk. SPSS will often take a column of numbers with one or more missing value codes, and convert it into a string. This makes it impossible for you to run most of the analyses that you would want to run.

You can tell SPSS during the import to designate the column of data as numeric, and SPSS will automatically convert any NA or asterisk to missing. Or you can convert from string to numeric after import. The conversion process will convert your NA and asterisk to missing.

If your dataset uses number codes for missing, you should have no trouble during import, but you do need to designate which numeric code or codes represents a missing value.

Don't forget this, or any statistics that you compute will be wrong, sometimes very wrong.
:::

## Imputing missing values, 1 of 2

-   Several simple (simplistic?) imputation choices
    -   No news is good news
    -   No news is bad news
    -   No news is average news
    -   No news is last week's news
    
## Imputing missing values, 2 of 2

-   Rigorous approaches (beyond the scope of this class)
    -   Maximum likelihood/Bayesian approaches
    -   Multiple imputation
-   You cannot avoid imputation

## SPSS analysis, 1 of 9

![](../images/sleep-analysis-01.png)

::: notes
Add note.
:::

## SPSS analysis, 2 of 9

![](../images/sleep-analysis-02.png)

::: notes
Add note.
:::

## SPSS analysis, 3 of 9

![](../images/sleep-analysis-03.png)

::: notes
Add note.
:::

## SPSS analysis, 4 of 9

![](../images/sleep-analysis-04.png)

::: notes
Add note.
:::

## SPSS analysis, 5 of 9

![](../images/sleep-analysis-05.png)

::: notes
Add note.
:::

## SPSS analysis, 6 of 9

![](../images/sleep-analysis-06.png)

::: notes
Add note.
:::

## SPSS analysis, 7 of 9

![](../images/sleep-analysis-07.png)

::: notes
Add note.
:::

## SPSS analysis, 8 of 9

![](../images/sleep-analysis-08.png)

::: notes
Add note.
:::

## SPSS analysis, 9 of 9

![](../images/sleep-analysis-09.png)

::: notes
Add note.
:::

## SPSS analysis, 10 of 9

![](../images/sleep-analysis-10.png)

::: notes
Add note.
:::

## SPSS analysis, 11 of 9

![](../images/sleep-analysis-11.png)

::: notes
Add note.
:::

## Partial correlation

-   $\rho_{XY\cdot Z}=\frac{\rho_{XY}-\rho_{XZ}\rho_{ZY}}   {\sqrt{1-\rho_{XZ}^2}\sqrt{1-\rho_{ZY}^2}}$

