---
title: "09-04, Alternative tests"
format: 
  revealjs:
    slide-number: true
    embed-resources: true
editor: source
---

## Alternative tests

-   If you have heterogeneity
    -   Welch's test
-   If you have non-normality
    -   Wilcoxon-Mann-Whitney test
-   If you have both:
    -   Log transformation
-   If you have lack of independence
    -   Random effects models
        -   Beyond the scope of this class

:::notes
There are several approaches that you should consider if you have trouble with the assumptions of the two-sample t-test.

Welch's test is a simple modification to the test if you have unequal variances. The Wilcoxon-Mann-Whitney test is a non-parametric test that does not require the assumption of normality.

If you have both heterogeneity and non-normality, and it has to be specific type of heterogeneity and non-normality, then a log transformation is good. It stretches the small values and squeezes the large values.

Lack of independence typically requires a fairly complex approach like a random effects model that is well beyond the scope of this class. This model is covered in quite a bit of detail by Anlin Cheng in MEDB 5503, Applied Biostatistics III.
:::
  
## Welch's test

-   $T = \frac{\bar{X}_1-\bar{X}_2}{se}$
-   se = standard error changes slightly
    -   $se = \sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}$
-   df = degrees of freedom changes slightly   
    -   $df=\frac{\big(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}\big)^2}{\frac{s_1^4}{(n_1-1)n_1^2}+\frac{s_2^4}{(n_2-1)n_2^2}}$
        -   Also known as Satterthwaite's approximation
-   R code
    -   `var.equal=FALSE`

:::notes
Welch's test makes minor modification to the standard error and the degrees of freedom. The formula for the degrees of freedom looks difficult, but it is just involves addition, division and raising to power. It is one of those formulas that has very little intuition and it is also something that makes us glad that the computer does all the work.
:::

## Mann-Whitney-Wilcoxon test

:::: {.columns}

::: {.column width="33%"}
![](../images/mann.jpg "Picture of Henry Mann")
Henry Mann, PhD in Mathematics in 1935
:::

::: {.column width="33%"}
![](../images/whitney.png)
Donald Ransom Whitney, PhD in Mathematics in 1946
:::

::: {.column width="33%"}
![](../images/wilcoxon.png "Picture of Frank Wilcoxon")
Frank Wilcoxon, PhD in Chemistry, 1924
:::

::::

:::notes

Here are images behind the three researchers who developed the Mann-Whitney-Wilcoxon test. The websites listed below also provide nice biographies of these three people.

Henry Mann: https://math.osu.edu/about-us/history/henry-berthold-mann
Donald Ransom Whitney: http://sections.maa.org/ohio/ohio_masters/whitney.pdf
Frank Wilcoxon: https://en.wikipedia.org/wiki/Frank_Wilcoxon
:::

## Other names

-   Wilcoxon-Mann-Whitney
-   Mann-Whitney U test
-   Wilcoxon rank sum test
    -   Not to be confused with Wilcoxon signed rank test
    
:::notes
There are alternate names for this test. Most people actually give Frank Wilcoxon first billing. It turns out that Frank Wilcoxon published a paper in 1945 that outlined both the rank sum test and the signed rank test. The rank sum test was derived under a limited assumption of equal sample sizes in the two groups. Mann and Whitney extended this test in 1947 to make it work with unequal sample sizes.
:::

## Theory behind the Mann-Whitney-Wilcoxon test

-   Combine the two groups
-   Assign ranks $R(X_{ij})$
    -   1 to smallest value, 2 to second smallest, etc.
-   Compute average rank, $\bar{R}$
-   Compute the sum of the ranks in first group, $\Sigma R(X_{1j})$
-   T = $\frac{\Sigma R(X_{1j})- n_1 \bar{R}}{se}$
    -   se = $\sqrt{\frac{n_1 n_2(n_1+n_2+1)}{12}}$
    
## Alternate theory

-   Count the times that $X_{1j}$ "wins" compared to all the $X_2$'s
    -   $X_{1j}$ wins if it is larger than $X_{2k}$
    -   Count ties as 0.5 (half of a "win")
    -   There are $n_1 n_2$ contests
    -   U = number of wins
    -   T = $\frac{U-\frac{n_1 n_2}{2}}{se}$
    -   Same se as earlier slide

## Hypothetical data

```{}
-   X1: 34, 1695, 1193
-   X2: 652, 11, 24, 16, 1543, 39
```

## Rank sum for hypothetical data

```{}
Group  Outcome  Rank  
    1       34     4
    1     1695     9
    1     1193     7
    2      652     6
    2       11     1
    2       24     3
    2       16     2
    2     1543     8
    2       39     5
```

-   $\Sigma R(X_{1j})$ = 4 + 9 + 7 = 20
-   Expected sum = 3*5 = 15

:::notes
Assign the rank of 1 to the smallest value (11), a rank of 2 to the second smallest value (16),..., and a rank of 9 to the largest value (1695)
:::

## Counting "wins" for hypothetical data

```{}
       652   11   24   16 1543   39
  34  Loss  Win  Win  Win Loss Loss
1695   Win  Win  Win  Win  Win  Win
1193   Win  Win  Win  Win Loss  Win
```

-   U = 14 (out of 18 possible)
-   Expected wins = 9


## The probability basis for ranking

![](../images/nine-ball.jpg)

::: notes
You randomly draw three balls from an urn containing balls numbered 1 through 9. What are the chances that you draw the a particular combination of these three balls?

:::

## List all the possible outcomes, 1

```{r}
n <- 9
m <- n*(n-1)*(n-2)/6
x <- rep(NA, m)
y <- rep(NA, m)
r <- 1:n
u <- 0
for (i in 1:(n-2)) {
  for (j in (i+1):(n-1)) {
    for (k in (j+1):n) {
      u <- u+1
      x[u] <- paste(r[i], r[j], r[k], sep=",")
      y[u] <- i+j+k
    }
  }
}
for(u in 0:11) {
  cat(x[(7*u+1):(7*u+7)], sep="  ")
  cat("\n")
}
```

::: notes
Here are all the possible outcomes. There are 84 outcomes and each has probability 1/84.
:::

## List all the possible outcomes, 2

```{r}
library(glue)
for (i in 6:14) {
	n <- i
	if (i < 10) n <- paste0(" ", i)
  cat(glue("{n}: "))
  cat(x[y==i], sep="  ")
  cat("\n")
}
```

## List all the possible outcomes, 3

```{r}
library(glue)
for (i in 15:24) {
	n <- i
	if (i < 10) n <- paste0(" ", i)
  cat(glue("{n}: "))
  cat(x[y==i], sep="  ")
  cat("\n")
}
```

## Log transformation

-   Arithmetic mean of original data
    -   $\frac{1}{n}\Sigma X_i$
-   Arithmetic mean of log transformed data
    -   $\frac{1}{n}\Sigma log(X_i)$
    -   $\frac{1}{n} log(X_1 \times X_2 \times ... \times X_n)$
-   Arithmetic mean of log transformed data back transformed
    -   $(X_1 \times X_2 \times\ ...\ \times X_n)^\frac{1}{n}$
    -   This is known as the geometric mean